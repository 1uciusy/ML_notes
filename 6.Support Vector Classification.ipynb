{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 支持向量机（分类）\n",
    "\n",
    "## 一、概念\n",
    "\n",
    "支持向量机是机器学习算法中比较有代表性的一个，它的本质思想是最大间隔分类器，也即我们的任务不仅是将样本分类，而且要使样本到分类线的距离最大。\n",
    "支持向量机的数学推导是机器学习算法中较为复杂的一个，很多互联网算法岗面试的重点之一就是支持向量机的推导（还有快速排序和时间复杂度）。\n",
    "我的数据挖掘老师曾表示，当你学习一门编程语言，如果你能用它实现快速排序，那么你可以算是了解了这门语言，如果你能实现支持向量机，那你可以算是熟悉了这门语言。\n",
    "\n",
    "支持向量机既可以用于分类任务也可以用于回归任务，在一些机器学习的python程序库中通常有SVC和SVR两个函数，其中SVC就是支持向量分类，SVR则是支持向量回归。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分类器\n",
    "支持向量分类适用于二分类任务，其标记数据的标签不是先前的$y\\in \\{ 0, 1\\}$而是$y\\in\\{-1,1\\}$。而分类器为：\n",
    "\n",
    "\\[\n",
    "h_{w,b} = sign(w^Tx+b)\\\\\n",
    "sign(z)=\\begin{cases}\n",
    "1, & z\\ge 0 \\\\\n",
    "0, & z <0\n",
    "\\end{cases}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.函数间隔和几何间隔\n",
    "\n",
    "定义给定的数据集中的样本（$x_i$，$y_i$）和超平面（w，b）的**函数间隔**为：\n",
    "\n",
    "\\[\n",
    "\\hat \\gamma_i = y_i(w^Tx=b)\n",
    "\\]\n",
    "\n",
    "而数据集和超平面的函数间隔则是数据集中所有样本和函数间隔的最小值：\n",
    "\n",
    "\\[\n",
    "\\hat \\gamma = \\underset{i=1,...,n}{max}\\hat \\gamma_i\n",
    "\\]\n",
    "\n",
    "支持向量分类的本意是最大间隔分类器，即是指希望间隔越大越好，因此当$y_i$是1时，希望$w^Tx+b$越大越好，当$y_i$是-1时，希望$w^Tx+b$（负数）越小越好。并且如果$y_i(x^Tx+b)>0$时，预测结果才是正确的。\n",
    "\n",
    "函数间隔可以表示预测的正确性，但并不能确切地反应样本点到超平面的距离。譬如（w,b）和（2w，2b）是同一个超平面，而函数间隔后者却是前者的二倍。\n",
    "\n",
    "因此需要将函数距离进行**单位化**，得到的就是**几何距离**：\n",
    "\n",
    "\\[\n",
    "\\gamma_i = y_i \\left( \\frac{w^T}{||w||}x_i+\\frac{b}{||w||}\\right)\n",
    "\\]\n",
    "\n",
    "整个样本的集合距离也就是：\n",
    "\n",
    "\\[\n",
    "\\gamma = \\underset{i=1,...n}{max} \\gamma_i\n",
    "\\]\n",
    "\n",
    "值得一提的是，上述间隔均是针对正确分类的样本，也即$y_i$和$w^Tx_i + b$同号。\n",
    "\n",
    "$\\frac b {||w||}$乍一看没有道理，但（2w，b）与（2w，2b）单位化之后并不相同，而（w，b）与（2w，2b）单位化之后相同，所以单位化时只需要除以w的L2范数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.间隔最大化\n",
    "\n",
    "支持向量机的目的是求的一个几何间隔最大的分离超平面，可以表示为以下有约束的最优化问题：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    & \\underset{w,b}{max} \\ \\ \\ \\ \\ \\gamma \\\\\n",
    "    & s.t. \\ \\ \\ \\ \\ y_i\\left(\\frac{w^T}{||w||}x_i + \\frac b {||w||}\\right) \\ge \\gamma, i=1,2,...n\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "根据几何间隔和函数间隔的关系$\\gamma = \\frac{\\hat \\gamma}{||w||}$\n",
    "\n",
    "则优化问题可以化为以函数间隔为表示的等价形式：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    & \\underset{w,b}{max} \\ \\ \\ \\ \\ \\frac{\\hat \\gamma}{||w||} \\\\\n",
    "    & s.t. \\ \\ \\ \\ \\ y_i\\left(w^T x_i + b \\right) \\ge \\hat \\gamma, i=1,2,...n\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "而根据之前对函数间隔的探讨，函数间隔的大小本身对优化问题没有影响，于是可以令$\\hat \\gamma=1$，优化问题又可以转化为等价形式：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    & \\underset{w,b}{min} \\ \\ \\ \\ \\ \\frac1 2 {||w||}^2 \\\\\n",
    "    & s.t. \\ \\ \\ \\ \\ y_i\\left(w^T x_i + b \\right) \\ge 1, i=1,2,...n\n",
    "\\end{aligned}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.拉格朗日对偶问题（Lagrange duality）\n",
    "\n",
    "先把之前的优化问题放在一边，考虑如下优化问题：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    & \\underset{w}{min} \\ \\ \\ \\ \\ f(w) \\\\\n",
    "    & s.t. \\ \\ \\ \\ \\ h_i(w) = 0, i=1,2,...n\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "这是微积分典型的条件极值，于是可以应用拉格朗日数乘法得到拉格朗日函数：\n",
    "\n",
    "\\[\n",
    "L(w, \\beta) = f(w) + \\sum_{i=1}^n \\beta_i h_i(w),i=1,2,...n\n",
    "\\]\n",
    "\n",
    "其中系数$\\beta_i$是拉格朗日乘子，求解拉格朗日的极值只对参数求偏导令其为零即可：\n",
    "\n",
    "\\[\n",
    "\\frac {\\partial L}{\\partial w_i}=0;\\ \\frac{\\partial L}{\\partial \\beta_i}=0\n",
    "\\]\n",
    "\n",
    "在此基础上加入含有不等式约束条件的优化问题，我们称之为**原始问题**：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\underset{w}{min} \\ \\ \\ \\ \\ & f(w) \\\\\n",
    "    s.t. \\ \\ \\ \\ \\ & h_i(w) = 0, i=1,2,...n \\\\\n",
    "                   & g_j(w)\\le 0, j=1,2,...m\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "为了求解这个问题，我们定义**广义拉格朗日数乘法**：\n",
    "\n",
    "\\[\n",
    "L(w, \\alpha, \\beta)=f(w)+\\sum_{j=1}^m \\alpha_jg_j(w) + \\sum_{i=1}^n \\beta_ih_i(w)\n",
    "\\]\n",
    "\n",
    "设原始问题的最大值为：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "\\Theta_P(w) &= \\underset{\\alpha, \\beta:\\alpha_i \\ge 0}{max} L(w,\\alpha, \\beta) \\\\\n",
    "    &= \\underset{\\alpha,\\beta:\\alpha_i \\ge 0}{max}f(w)+\\sum_{j=1}^m \\alpha_jg_j(w) + \\sum_{i=1}^n \\beta_ih_i(w)\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "易知，当w满足约束条件时，最大值为$f(w)$也就是目标函数，当w不满足条件时最大值是$\\infty$。因此当我们考虑最小化$\\Theta_P$时 我们会发现这和原始问题是同一个问题，或者是同解的。\n",
    "\n",
    "\\[\n",
    "\\underset{w}{min}\\ \\Theta_P(w) = \\underset{w}{min}\\underset{\\alpha, \\beta:\\alpha_i \\ge 0}{max} L(w,\\alpha, \\beta)\n",
    "\\]\n",
    "\n",
    "定义对偶问题为：\n",
    "\n",
    "\\[\n",
    "\\Theta_D(\\alpha, \\beta) = \\underset{w}{min}L(w, \\alpha, \\beta)\n",
    "\\]\n",
    "\n",
    "对偶的优化问题为：\n",
    "\n",
    "\\[\n",
    "\\underset{\\alpha, \\beta:\\alpha_i \\ge 0}{max} \\Theta_D(\\alpha, \\beta) = \\underset{\\alpha, \\beta:\\alpha_i \\ge 0}{max} \\underset{w}{min} L(w, \\alpha, \\beta)\n",
    "\\]\n",
    "\n",
    "由于max(min)总是小于等于min(max)，譬如下面这个例子：\n",
    "\n",
    "\\[\n",
    "\\underset{y\\in\\{0, 1\\}}{max}\\left(\\underset{x\\in\\{0, 1\\}}{min}I(x=y)\\right)\\le \\underset{x\\in\\{0, 1\\}}{min}\\left(\\underset{y\\in\\{0, 1\\}}{max}I(x=y)\\right)\n",
    "\\]\n",
    "\n",
    "于是就有了对偶问题的解小于等于原始问题的解：\n",
    "\n",
    "\\[\n",
    "d^* = \\underset{\\alpha, \\beta:\\alpha_i \\ge 0}{max} \\underset{w}{min} L(w, \\alpha, \\beta) \\le \\underset{w}{min}\\underset{\\alpha, \\beta:\\alpha_i \\ge 0}{max} L(w,\\alpha, \\beta) = p^*\n",
    "\\]\n",
    "\n",
    "等号成立的条件便是**KKT条件（Karush-Kuhn-Tucker condition）**：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial}{\\partial w_i}L(w,\\alpha, \\beta) &= 0, i=1,2,...,n\\\\\n",
    "    \\frac{\\partial}{\\partial \\beta_i}L(w,\\alpha, \\beta) &= 0, i=1,2,...,p\\\\\n",
    "    \\alpha_ig_i(w) &=0, i = 1,2,...,q\\\\\n",
    "    g_i(w) &\\le 0, i = 1,2,...,q\\\\\n",
    "    \\alpha_i &\\le 0, i = 1,2,...,q\\\\\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "值得一提的是，原始问题是先求解关于α，β的函数最大值，得到的是关于W的函数，再进行求解；而对偶问题是先求解关于W的函数，得到的是关于于α，β的函数，再进行求解。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.最优间隔分类器\n",
    "\n",
    "根据前述的推广的拉格朗日数乘法，原始优化问题可以写成以下形式：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\underset{\\gamma, w, b}{min}\\ \\ \\ & \\frac12 {||w||}^2 \\\\\n",
    "    s.t.\\ \\ \\ & g_i(w) = -y_i(w^T x_i + b) + 1\\le 0, i=1,2,...,n\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "根据KKT条件第三个等式，事实上只有函数间隔为最小值1的样本点$\\alpha_i$不为0，这些样本点即被称为支持向量。\n",
    "\n",
    "此问题的拉格朗日函数则是：\n",
    "\n",
    "\\[\n",
    "L(w, b, \\alpha) = \\frac12 {||w||}^2 - \\sum_{i=1}^n \\alpha_i [y_i(w^T x_i + b) - 1]\n",
    "\\]\n",
    "\n",
    "拉格朗日函数对w，b求梯度再令其为零：\n",
    "\n",
    "\\[\n",
    "\\triangledown_w L(w, b, \\alpha) = w - \\sum_{i=1}^n \\alpha_i y_i x_i \\\\\n",
    "w^* = \\sum_{i=1}^n \\alpha_i y_i x_i \\\\\n",
    "\\frac{\\partial}{\\partial b}L(w, b, \\alpha) = \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\]\n",
    "\n",
    "将$w^*$回代到公式（22）：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    L(w^*, b, \\alpha) &= \\frac12 w^{*T}w^* - \\sum_{i=1}^n \\alpha_i [y_i(w^T x_i + b) - 1] \\\\\n",
    "        &= \\frac12 \\left(\\sum_{i=1}^n \\alpha_i y_i x_i\\right)^T \\left(\\sum_{i=1}^n \\alpha_i y_i x_i\\right) - \\sum_{i=1}^n \\alpha_i \\left[y_i(\\left(\\sum_{i=1}^n \\alpha_i y_i x_i\\right)^T x_i + b) - 1\\right] \\\\\n",
    "        &= \\frac12 \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j \\alpha_i \\alpha_j x_i^T x_j - \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j \\alpha_i \\alpha_j x_i^T x_j + \\sum_{i=1}^n \\alpha_i - \\sum_{i=1}^n \\alpha_i y_i b \\\\\n",
    "        & = \\sum_{i=1}^n \\alpha_i - \\frac12 \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j \\alpha_i \\alpha_j x_i^T x_j\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "此时的优化问题变成：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\underset{\\alpha}{max}\\ \\  &W(\\alpha) = \\sum_{i=1}^n \\alpha_i - \\frac12 \\sum_{i=1}^n \\sum_{j=1}^n y_i y_j \\alpha_i \\alpha_j x_i^T x_j\\\\\n",
    "        s.t.\\ \\  & \\alpha_i \\ge 0, i=1,2,...,n\\\\\n",
    "        & \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "这就印证了我们之前对对偶问题的描述，即先求关于W的函数的极值，得到关于α和β的函数，再求解极值。\n",
    "\n",
    "b的求解则是通过反证法获得：如果所有的$\\alpha_i$都等于0，则根据公式（21）$W=0$，分类器就变成$f(x)=sign(W^Tx + b) = sign(b)$，对于任意新的输入，输出恒为+1或-1。于是必有$\\alpha_i > 0$，也即支持向量一定存在。则对任意的$\\alpha_j > 0$，根据$g_i(w)$的定义，$b^*=y_j - \\sum_{i=1}^n \\alpha_i y_i x_i^T x_j$。又根据先前的公式（）我们可以知道对b求导的结果恒为零，故b为一常数，对于任意支持向量$b^*=y_j - \\sum_{i=1}^n \\alpha_i y_i x_i^T x_j$结果将是一样的。\n",
    "\n",
    "$b^*$另有一种解：\n",
    "\n",
    "\\[\n",
    "b^* = -\\frac{\\underset{i:y_i = -1}{max}W^{*T}x_i + \\underset{i:y_i = 1}{min}W^{*T}x_i}{2}\n",
    "\\]\n",
    "\n",
    "这是斯坦福CS229中给到的一种解法，根据结构推测是将一个正例得来的解和一个负例得来的解进行平均得到的。\n",
    "\n",
    "假设我们得到了参数$W^*,b^*$，当我们用支持向量机预测时：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    W^{*T}x + b^* &= \\left(\\sum_{i=1}^n \\alpha_i y_i x_i\\right)^T x + b^* \\\\\n",
    "        &= \\sum_{i=1}^n \\alpha_i y_i x_i^Tx + b^*\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "由于之前讨论过，只有支持向量的拉格朗日乘数$\\alpha_i$大于零，其余的拉格朗日乘数均为零，所以预测时将会减少很多不必要的计算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.核（Kernels）\n",
    "\n",
    "除了优化求解W和b的算法外，线性可分支持向量机的推导已经全部完成，针对用非线性分类器可以完全分类的数据，则需要核方法。核方法的本质就是特征工程，将特征映射到更高维的空间上，为数据加入非线性的因素。\n",
    "\n",
    "假设有如下映射$\\phi(x)$：\n",
    "\\[\n",
    "\\phi(x) = \\begin{bmatrix}\n",
    "x\\\\\n",
    "x^2\\\\\n",
    "x^3\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "则核定义如下：\n",
    "\n",
    "\\[\n",
    "K(x,z) = \\phi(x)^T\\phi(z)\n",
    "\\]\n",
    "\n",
    "这是从映射到核函数的方法，另有从核函数到映射的方法：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    K(x,z) &= (x^Tz)^2\\\\\n",
    "        &= \\left(\\sum_{i=1}^nx_iz_i\\right)\\left(\\sum_{i=1}^nx_iz_i\\right)\\\\\n",
    "        &= \\sum_{i=1}^n \\sum_{j=1}^n x_i x_j z_i z_j\\\\\n",
    "        &= \\sum_{i,j=1}^n (x_i x_j)(z_i z_j)\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "对应的映射则是：\n",
    "\n",
    "\\[\n",
    "\\phi(x)=\\begin{bmatrix}\n",
    "x_1x_1\\\\\n",
    "x_1x_2\\\\\n",
    "x_1x_3\\\\\n",
    "x_2x_1\\\\\n",
    "x_2x_2\\\\\n",
    "x_2x_3\\\\\n",
    "x_3x_1\\\\\n",
    "x_3x_2\\\\\n",
    "x_3x_3\n",
    "\\end{bmatrix}\n",
    "\\]\n",
    "\n",
    "这样的映射通常是很难构造或者难以想到的，而这样的核函数却往往是在情理之中或者易于想到的，而且诸如此来的核函数不必计算映射再求向量内积，而是直接在原始向量上进行计算，于是对于高维数据而言，内存和时间得到了兼顾。\n",
    "\n",
    "应用核方法时只需要将所有的$X^T_iX_j$替换成相应的核函数$K(X^T_i,X_j)$即可。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.正则化和不可分情形\n",
    "\n",
    "无论线性还是非线性，以上讨论的都是数据完全可分的情况。然而这样的实际数据少之又少，哪怕映射到更高维的空间之后，不能完全可分的案例也比比皆是。有时即便是可以线性可分，但一些极端样本点的出现会使支持向量的几何间隔大大减小，这种情况就弱化了支持向量机最大分类间隔的本意，为了应对以上问题，我们便提出了正则化的思想，对目标函数进行一定的“惩罚”，优化问题就变成：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\underset{\\gamma, w, b}{min}\\ \\ &\\frac12 {||w||}^2+C\\sum_{i=1}^n \\xi_i \\\\\n",
    "    s.t. \\ \\ &y_i(w^T x + b) > 1-\\xi_i, i=1,2,...,n\\\\\n",
    "    &\\xi_i \\ge 0, i=1,2,...,n\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "其中$\\xi_i$是针对每个样本加的约束，它的存在允许函数间隔小于1，同时又会使目标函数增加$C\\xi_i$，由于目标是最小化目标函数，所以这是某种意义上的“损失”或者说“惩罚”。C则是一个超参数，权衡了函数间隔和目标函数的关系。C越大，惩罚作用越强，函数间隔小于1的越少；C越小，惩罚作用越弱，函数间隔小于1的越多。\n",
    "\n",
    "对应的拉格朗日函数就是：\n",
    "\n",
    "\\[\n",
    "L(w, b, \\xi, \\alpha, r) = \\frac12 w^Tw + C\\sum_{i=1}^n\\xi_i - \\sum_{i=1}^n\\alpha_i[y_i(w^Tx+b)-1+\\xi_i]-\\sum_{i=1}^nr_i\\xi_i\n",
    "\\]\n",
    "\n",
    "前几项好理解，最后一项$-\\sum_{i=1}^nr_i\\xi_i$则是因为$\\xi$本身也是变量，所以在拉格朗日函数里也需要对它进行约束。\n",
    "\n",
    "其对偶问题为：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\underset{\\alpha}{max}\\ \\  &W(\\alpha)=\\sum_{i=1}^n\\alpha_i - \\frac12 \\sum_{i,j=1}^n y_i y_j \\alpha_i \\alpha_j x^T_i x_j\\\\\n",
    "    s.t.\\ \\ & 0\\le \\alpha_i\\le C,i=1,2,...,n \\\\\n",
    "    & \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "关于α不能大于C可以参见拉格朗日函数，将中间两项合并，得到$-\\sum_{i=1}^n\\{\\alpha_i[y_i(w^Tx+b)-1]+(\\alpha_i-C)\\xi_i\\}$，若α大于C则相当于使函数间隔大于一，这与我们施加惩罚项允许函数间隔小于1的本意背道而驰。\n",
    "\n",
    "类似于可完全分类的情况下的KKT条件第三个等式，在非完全可分条件下也有类似的关系：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\alpha_i=0\\ \\ &\\Rightarrow\\ \\ y_i(w^Tx_i + b) \\ge 1\\\\\n",
    "    \\alpha_i=C\\ \\ &\\Rightarrow\\ \\ y_i(w^Tx_i + b) \\le 1\\\\\n",
    "    0<\\alpha_i<0\\ \\ &\\Rightarrow\\ \\ y_i(w^Tx_i + b) = 1\\\\\n",
    "\\end{aligned}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.SMO算法\n",
    "\n",
    "迄今为止我们已经讨论了线性可完全分类、非线性可完全分类、（非）线性不可完全分类的情况，除了如何求解$w^{*T}$之外（b可以在此基础上求出），其他的数学推导已经完成。\n",
    "\n",
    "最后我们要推导的即是如何求解参数。\n",
    "\n",
    "#### 1）坐标上升法（Coordinate ascent）\n",
    "\n",
    "坐标上升法是一种简单直接的优化方法，对于无约束的优化问题：\n",
    "\n",
    "\\[\n",
    "\\underset{\\alpha}{max}\\ \\ W(\\alpha_1, \\alpha_2,...,\\alpha_n)\n",
    "\\]\n",
    "\n",
    "W是关于α的函数，坐标上升则是针对各个变量分别作为唯一变量求最值，循环至收敛：\n",
    "\n",
    "Loop until convergence:{\n",
    "\n",
    "    For i = 1,...,m,{\n",
    "        α_i := argmax_{α_i} W(α_1,α_2,...,α_m)\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "#### 2)SMO（sequential minimal optimization）\n",
    "\n",
    "我们想要求解的优化问题为：\n",
    "\n",
    "\\[\n",
    "\\begin{aligned}\n",
    "    \\underset{\\alpha}{max}\\ \\  &W(\\alpha)=\\sum_{i=1}^n\\alpha_i - \\frac12 \\sum_{i,j=1}^n y_i y_j \\alpha_i \\alpha_j x^T_i x_j\\\\\n",
    "    s.t.\\ \\ & 0\\le \\alpha_i\\le C,i=1,2,...,n \\\\\n",
    "    & \\sum_{i=1}^n \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
