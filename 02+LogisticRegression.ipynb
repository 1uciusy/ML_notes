{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression\n",
    "\n",
    "## 一、概念\n",
    "\n",
    "逻辑回归是应用于**二分类**的方法。相对的应用于多分类的方法叫softmax。\n",
    "\n",
    "逻辑回归的实现依赖于sigmoid函数：\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "可以看出这是一个单调递增，当x趋向于负无穷时函数值趋向于0，x趋向于正无穷时函数值趋向于1，对应于某一样本属于某一类的概率。\n",
    "\n",
    "对于一个有p个变量的样本而言，模型的形式为：\n",
    "\n",
    "$$\n",
    "\\hat y_i = P(y_i = 1) = \\frac{1}{1+e^{-(\\theta_0 + \\theta_1 x_1+...+\\theta_p x_p)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、损失函数\n",
    "\n",
    "逻辑回归的损失函数是交叉熵，从数理统计的角度出发，则是极大似然函数。\n",
    "\n",
    "假设样本来自于伯努利分布，$y_i\\in \\{ 0, 1\\}$，对概率的估计值$\\hat y_i\\in (0,1)$，则估计的似然函数是：\n",
    "\n",
    "$$\n",
    "L =\\prod_{i=1}^n {\\hat {y_i}}^{y_i}(1-\\hat {y_i})^{1-{y_i}} \n",
    "$$\n",
    "\n",
    "对其求以自然底数为底数的对数：\n",
    "\n",
    "$$\n",
    "ln(L) = \\sum_{i=1}^n (y_i ln(\\hat {y_i})+(1-{y_i})ln(1-\\hat {y_i}))\n",
    "$$\n",
    "\n",
    "根据极大似然估计的定义，似然函数的函数值越大，表示随机事件更可能服从当前的概率分布。\n",
    "由于伯努利分布的似然函数值在0，1之间，取对数后不改变增减性，极大化似然函数则等价于极小化损失函数。\n",
    "\n",
    "因此损失函数就定义为：\n",
    "\n",
    "$$\n",
    "loss = -\\frac 1 m\\sum_{i=1}^n y_i ln(\\hat {y_i})+(1-{y_i})ln(1-\\hat {y_i}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、优化\n",
    "\n",
    "和多元线性回归类似，逻辑回归也存在相应假定\n",
    "\n",
    "尽管逻辑回归的形式像是把多元线性回归套进sigmoid函数，但逻辑回归没有像多元线性回归那样的正规方程（据我所知没有），因此逻辑回归的参数求解和优化主要靠梯度下降。\n",
    "\n",
    "梯度下降的核心在于求导，样本为常数，参数为变量，把损失函数视为参数的函数对其求导。\n",
    "\n",
    "首先，由于真实值已知，所以损失函数是关于估计值的函数：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial \\hat y_i} = -\\frac {1}{n}(\\frac {y_i}{\\hat y_i} - \\frac{1-y_i}{1-\\hat y_i})\n",
    "$$\n",
    "\n",
    "而$\\hat y_i$是数据各变量线性组合$z_i$的sigmoid函数：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\hat y_i &= \\frac{1}{1+e^{-z_i}} \\\\\n",
    "    \\frac{\\partial \\hat y_i}{\\partial z_i} &= \\frac{e^{-z_i}}{(1+e^{-z_i})^2} = \\frac{1}{1+e^{-z_i}}(1-\\frac{1}{1+e^{-z_i}})=\\hat y_i(1-\\hat y_i)\\\\\n",
    "    z_i &= \\theta_0 + \\theta_1 x_1+...+\\theta_p x_p \\\\\n",
    "    \\frac{\\partial z_i}{\\partial \\theta_j} &= x_{ij}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "接下来求梯度有两种方法：\n",
    "1. 求分量的导数：\n",
    "$$\n",
    "\\frac{\\partial loss}{\\partial \\theta_j} = \\sum_{i=1}^n \\frac{\\partial loss}{\\partial \\hat y_i} \\frac{\\partial \\hat y_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial \\theta_j} = \\frac 1 n \\sum_{i=1}^n(\\hat y_i - y_i)x_{ij} \\\\\n",
    "$$\n",
    "然后迭代：\n",
    "$$\n",
    "\\theta_j := \\theta_j - step\\times\\frac 1 n \\sum_{i=1}^n(\\hat y_i - y_i)x_{ij}\n",
    "$$\n",
    "2. 直接求梯度矩阵\n",
    "\n",
    "损失是一个标量，$\\hat y_i$，$z_i$和$\\Theta$分别是$n\\times 1$，$n\\times 1$和$p\\times 1$维向量，根据雅可比（Jacob）矩阵的定义，最终的梯度$\\frac {\\triangledown loss}{\\triangledown \\Theta}$应该是一个$p\\times 1$维的向量。\n",
    "\n",
    "首先，损失对估计值向量的导数（梯度）是一个雅可比矩阵：\n",
    "\n",
    "$$\n",
    "\\frac{\\triangledown loss}{\\triangledown \\hat Y} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial loss}{\\partial \\hat y_1} \\\\\n",
    "\\frac{\\partial loss}{\\partial \\hat y_2} \\\\\n",
    "... \\\\\n",
    "\\frac{\\partial loss}{\\partial \\hat y_n}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "-\\frac {1}{n}(\\frac {y_1}{\\hat y_1} - \\frac{1-y_1}{1-\\hat y_1}) \\\\\n",
    "-\\frac {1}{n}(\\frac {y_2}{\\hat y_2} - \\frac{1-y_2}{1-\\hat y_2}) \\\\\n",
    "... \\\\\n",
    "-\\frac {1}{n}(\\frac {y_n}{\\hat y_n} - \\frac{1-y_n}{1-\\hat y_n})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其次，$\\hat Y$和$Z$的函数关系是分量之间的函数关系，于是：\n",
    "\n",
    "$$\n",
    "\\frac{\\triangledown \\hat Y}{\\triangledown Z} = \n",
    "\\begin{bmatrix}\n",
    "\\hat y_1(1-\\hat y_1)\\\\\n",
    "\\hat y_2(1-\\hat y_2)\\\\\n",
    "...\\\\\n",
    "\\hat y_n(1-\\hat y_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "然后，$Z=X\\Theta$需要求矩阵对矩阵的导数，其中的数学很复杂，我比较赞成的是这个方法：https://www.zhihu.com/question/39523290/answer/100057066 ，即导数的矩阵形式按需自取，也可以说是“凑”，先前求得的导数都是$n\\times1$维的，因此只需一个$p\\times n$维的矩阵参与，即可求得合适的梯度，即：\n",
    "\n",
    "$$\n",
    "\\frac{\\triangledown Z}{\\triangledown \\Theta} = X^T\n",
    "$$\n",
    "\n",
    "总结起来：\n",
    "\n",
    "$$\n",
    "\\frac{\\triangledown loss}{\\triangledown \\Theta} = \\frac{\\triangledown Z}{\\triangledown \\Theta}(\\frac{\\triangledown loss}{\\triangledown \\hat Y}*\\frac{\\triangledown \\hat Y}{\\triangledown Z}) = X^T\\begin{bmatrix}\n",
    "-\\frac 1 n (y_1 - \\hat y_1)\\\\\n",
    "-\\frac 1 n (y_2 - \\hat y_2)\\\\\n",
    "...\\\\\n",
    "-\\frac 1 n (y_n - \\hat y_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "其中\\*是按元素相乘。\n",
    "\n",
    "最后按照梯度下降的定义进行迭代即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、应用\n",
    "\n",
    "选用的是数据集是经典的鸢尾花数据集：http://archive.ics.uci.edu/ml/datasets/Iris ，一共五列变量，分别是花萼长度、花萼宽度、花瓣长度、花瓣宽度和鸢尾花的品种，数据简洁易于快速应用算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T08:53:33.326339Z",
     "start_time": "2019-03-12T08:53:31.323684Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>sepal.l</th>\n",
       "      <th>sepal.w</th>\n",
       "      <th>petal.l</th>\n",
       "      <th>petal.w</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  sepal.l  sepal.w  petal.l  petal.w        class\n",
       "0     1      5.1      3.5      1.4      0.2  Iris-setosa\n",
       "1     1      4.9      3.0      1.4      0.2  Iris-setosa\n",
       "2     1      4.7      3.2      1.3      0.2  Iris-setosa\n",
       "3     1      4.6      3.1      1.5      0.2  Iris-setosa\n",
       "4     1      5.0      3.6      1.4      0.2  Iris-setosa"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['sepal.l','sepal.w','petal.l','petal.w','class']\n",
    "iris = pd.read_csv('data_set/Iris.data', header=None, names=cols)\n",
    "iris.insert(0, 'bias', 1)\n",
    "iris.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris数据集中有三类花的数据，logistic regression适用于二分类，因此我们从Iris中选取两种花的数据作为训练数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:37:57.852080Z",
     "start_time": "2019-02-24T06:37:57.838083Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.index = iris['class']\n",
    "iris['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:38:02.073586Z",
     "start_time": "2019-02-24T06:38:02.056516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = iris.loc[['Iris-setosa', 'Iris-versicolor'],:]\n",
    "iris['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:39:13.335928Z",
     "start_time": "2019-02-24T06:39:13.242473Z"
    }
   },
   "outputs": [],
   "source": [
    "n = iris.shape[0] # 样本数\n",
    "p = iris.shape[1]-1 # 变量数\n",
    "\n",
    "np.random.seed(2099)\n",
    "index = np.random.permutation(n) # 打乱样本索引\n",
    "\n",
    "train_index = index[0: int(0.7*n)]\n",
    "test_index = index[int(0.7*n): n]\n",
    "\n",
    "y = iris['class']\n",
    "y = y=='Iris-setosa'\n",
    "x = iris.drop(['class'], axis=1)\n",
    "\n",
    "train_x = x.iloc[train_index]\n",
    "train_y = y.iloc[train_index]\n",
    "test_x = x.iloc[test_index]\n",
    "test_y = y.iloc[test_index]\n",
    "\n",
    "train_y = np.array(train_y).reshape([int(0.7*n), 1])\n",
    "test_y = np.array(test_y).reshape([n-int(0.7*n), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:39:40.071183Z",
     "start_time": "2019-02-24T06:39:40.059206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24336871]\n",
      " [-1.07204116]\n",
      " [-0.06854123]\n",
      " [-0.44725433]\n",
      " [ 0.4089976 ]]\n"
     ]
    }
   ],
   "source": [
    "def initialize(p):\n",
    "    return np.random.randn(p).reshape([p, 1])\n",
    "\n",
    "theta = initialize(p)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:39:42.918027Z",
     "start_time": "2019-02-24T06:39:42.908040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18168242e-04]\n",
      " [1.19431968e-03]\n",
      " [1.82891796e-03]\n",
      " [1.29628065e-03]\n",
      " [5.16319355e-04]\n",
      " [2.08947873e-04]\n",
      " [7.74229880e-04]\n",
      " [1.47235670e-03]\n",
      " [3.88988777e-04]\n",
      " [4.18409328e-04]\n",
      " [2.72819821e-04]\n",
      " [1.80808517e-03]\n",
      " [1.28280893e-03]\n",
      " [1.33698913e-04]\n",
      " [2.06605462e-03]\n",
      " [7.53311575e-04]\n",
      " [2.06030984e-04]\n",
      " [1.50865120e-03]\n",
      " [1.08666180e-04]\n",
      " [4.28638128e-04]\n",
      " [2.07475411e-04]\n",
      " [1.61678645e-03]\n",
      " [1.71438708e-04]\n",
      " [4.46981250e-04]\n",
      " [1.67512631e-03]\n",
      " [2.65918245e-04]\n",
      " [1.58908648e-03]\n",
      " [5.39683701e-04]\n",
      " [3.45110196e-03]\n",
      " [5.23442192e-04]\n",
      " [1.83672973e-03]\n",
      " [2.16082610e-03]\n",
      " [2.62919074e-03]\n",
      " [9.37060879e-04]\n",
      " [1.01036263e-03]\n",
      " [7.65099459e-04]\n",
      " [1.19288691e-04]\n",
      " [2.53172907e-03]\n",
      " [4.57807848e-04]\n",
      " [4.03040687e-03]\n",
      " [1.43730724e-04]\n",
      " [1.00974263e-03]\n",
      " [1.57153490e-03]\n",
      " [1.85718071e-04]\n",
      " [1.03215318e-03]\n",
      " [5.25866338e-04]\n",
      " [1.95491545e-03]\n",
      " [7.08187601e-04]\n",
      " [2.16011534e-04]\n",
      " [2.55116949e-04]\n",
      " [1.14881894e-03]\n",
      " [1.19571914e-03]\n",
      " [1.45266640e-03]\n",
      " [3.53491311e-04]\n",
      " [1.54426209e-03]\n",
      " [1.93376524e-03]\n",
      " [4.81962211e-04]\n",
      " [8.01941035e-05]\n",
      " [3.70775033e-04]\n",
      " [3.90393587e-04]\n",
      " [2.47034216e-03]\n",
      " [2.24176665e-03]\n",
      " [1.14327589e-03]\n",
      " [2.70146749e-03]\n",
      " [1.91520442e-03]\n",
      " [3.12368695e-04]\n",
      " [1.35169580e-03]\n",
      " [2.75026341e-04]\n",
      " [1.67443275e-04]\n",
      " [1.67774987e-03]]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x, theta):\n",
    "    return 1/(1 + np.exp(-np.dot(x, theta)))\n",
    "\n",
    "train_y_hat = sigmoid(train_x, theta)\n",
    "print(train_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:41:08.923299Z",
     "start_time": "2019-02-24T06:41:08.911773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.77718095]]\n"
     ]
    }
   ],
   "source": [
    "def loss(y, y_hat):\n",
    "    n = y.shape[0]\n",
    "    return -1/n*(np.dot(y.T, np.log(y_hat)) + np.dot(1-y.T, np.log(1-y_hat)))\n",
    "\n",
    "train_loss = loss(train_y, train_y_hat)\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:53:29.449561Z",
     "start_time": "2019-02-24T06:53:28.973048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03819359]]\n"
     ]
    }
   ],
   "source": [
    "def train(y, x, theta, step=0.1, delta = 0.0001):\n",
    "    decay = 1\n",
    "    loss_pre = None\n",
    "    loss_aft = float('inf')\n",
    "    y_hat = sigmoid(x, theta)\n",
    "    \n",
    "    while decay > delta:\n",
    "        gradient = np.dot(x.T, 1/n*(y_hat - y))\n",
    "        \n",
    "        theta -= step*gradient\n",
    "        y_hat = sigmoid(x, theta)\n",
    "        \n",
    "        loss_pre = loss_aft\n",
    "        loss_aft = loss(y, y_hat)\n",
    "        decay = loss_pre-loss_aft\n",
    "        \n",
    "        #print(decay)\n",
    "    \n",
    "    return theta, loss_aft\n",
    "\n",
    "theta, train_loss = train(train_y, train_x, theta)\n",
    "print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:55:19.614144Z",
     "start_time": "2019-02-24T06:55:19.602632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04520658]]\n"
     ]
    }
   ],
   "source": [
    "test_y_hat = sigmoid(test_x, theta)\n",
    "test_loss = loss(test_y, test_y_hat)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T06:40:16.420407Z",
     "start_time": "2019-02-24T06:40:16.414407Z"
    }
   },
   "source": [
    "## 五、阈值和评价指标\n",
    "\n",
    "在分类问题中，我们求出的估计值通常是一个概率值，是属于0到1的连续实数$(0,1)$，而真实值的取值集合$\\{ 0, 1\\}$。\n",
    "因此我们通常需要选取阈值，即确定概率值大于多少时认为取值是1，反之取值为0。\n",
    "同时，直接的损失函数并不直观，通常我们还用准确率和一些其他评价指标。\n",
    "\n",
    "### 1.阈值的选取\n",
    "\n",
    "### 2.评价指标\n",
    "\n",
    "最常见的评价指标是准确率，$accuracy = \\frac 1 n \\sum_{i=1}^n I(\\hat y_i = y_i)$，准确率非常直观，即预测结果正确的占比。\n",
    "\n",
    "尽管准确率较直观，但仍然有些粗糙，除了准确率之外我们还有查准率、召回率和F1分数作为评价指标。\n",
    "\n",
    "| 混淆矩阵 | 预测正例-1  | 预测反例-0  |\n",
    "|----------|:-----------:|:-----------:|\n",
    "| 真实正例 | TP（真正例）| FN（假反例）|\n",
    "| 真实反例 | FP（假正例）| TN（真反例）|\n",
    "\n",
    "- TP：预测为真，实际为真\n",
    "- FP：预测为真，实际为假\n",
    "- FN：预测为假，实际为真\n",
    "- TN：预测为假，实际为假\n",
    "\n",
    "查准率： $P = precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "召回率： $R = recall = \\frac{TP} {TP + FN}$\n",
    "\n",
    "F1分数则是二者的调和平均数：$F1 = \\frac{2}{\\frac 1 P + \\frac 1 R}$\n",
    "\n",
    "根据不同的问题选用不同的指标是较为明智的选择，譬如犯罪嫌疑人识别，我们希望尽可能多得识别，即模型判断是犯罪嫌疑人的人数占所有犯罪嫌疑人人数的比例尽可能高，此时选用查准率；而疾病检测则不同，误诊会给病人带来极大的困扰，因此我们“希望”诊断患有某种疾病的人“确实”患有该疾病，此时选用召回率更合适；但也有很多情况中，并没有明显的偏好，因此选用F1分数将会是能够兼顾二者的一个较好选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T07:49:00.229741Z",
     "start_time": "2019-02-24T07:49:00.209743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on train set:1.0\n",
      "accuracy on test set:1.0\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y, y_hat, threshold=0.5):\n",
    "    y_hat = y_hat > threshold\n",
    "    return np.sum(y == y_hat)/y.shape[0]\n",
    "\n",
    "train_y_hat = sigmoid(train_x, theta)\n",
    "train_accuracy = accuracy(train_y, train_y_hat)\n",
    "test_accuracy = accuracy(test_y, test_y_hat)\n",
    "\n",
    "print('accuracy on train set:'+str(train_accuracy))\n",
    "print('accuracy on test set:'+str(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于数据量较少，纬度较低，因此准确率较高，下面计算查准率、召回率和F1分数（尽管根据准确率来看，三者肯定都是1）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T08:05:24.859831Z",
     "start_time": "2019-02-24T08:05:24.851838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def precision(y, y_hat):\n",
    "    TP_FP_index = np.where(y_hat == 1)[0]\n",
    "    TP_FP = len(TP_FP_index)\n",
    "    \n",
    "    TP = np.sum(y_hat[TP_FP_index] == y[TP_FP_index])\n",
    "    \n",
    "    return TP/TP_FP\n",
    "\n",
    "test_y_hat = test_y_hat > 0.5\n",
    "test_precision = precision(test_y, test_y_hat)\n",
    "print(test_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T08:06:57.399752Z",
     "start_time": "2019-02-24T08:06:57.383750Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def recall(y, y_hat):\n",
    "    TP_FN_index = np.where(y == 1)[0]\n",
    "    TP_FN = len(TP_FN_index)\n",
    "    \n",
    "    TP = np.sum(y[TP_FN_index] == y_hat[TP_FN_index])\n",
    "    \n",
    "    return TP/TP_FN\n",
    "\n",
    "test_recall = recall(test_y, test_y_hat)\n",
    "print(test_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-24T08:08:20.227911Z",
     "start_time": "2019-02-24T08:08:20.215905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def F1(y, y_hat):\n",
    "    return 2/(1/precision(y, y_hat) + 1/recall(y, y_hat))\n",
    "\n",
    "test_F1 = F1(test_y, test_y_hat)\n",
    "print(test_F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、实际应用\n",
    "\n",
    "在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：\n",
    "\n",
    "0. 离散特征的增加和减少都很容易，易于模型的快速迭代；\n",
    "\n",
    "1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；\n",
    "\n",
    "2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；\n",
    "\n",
    "3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；\n",
    "\n",
    "4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；\n",
    "\n",
    "5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；\n",
    "\n",
    "6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。\n",
    "\n",
    "李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。\n",
    "\n",
    "来源：知乎：https://www.zhihu.com/question/31989952/answer/54184582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
