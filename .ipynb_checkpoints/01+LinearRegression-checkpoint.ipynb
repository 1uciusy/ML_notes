{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多元回归\n",
    "\n",
    "## 一、概念\n",
    "\n",
    "回归（regression）是一种估计变量关系的统计方法。\n",
    "\n",
    "通过回归可以建立因变量（$Y$）和自变量（$X$）的数量关系，以此进行预测、控制。\n",
    "\n",
    "因变量（$Y$）通常是连续实数，自变量（$X$）可以是连续的也可以是离散的。\n",
    "\n",
    "自变量和因变量之间的关系用矩阵进行刻画：$Y = X\\beta + \\epsilon$，其中$Y$是$n\\times 1$的向量，n代表样本数，$X$是$n\\times (p+1)$维样本矩阵，p是自变量个数，额外一列是偏置项$x_{i0}=1$，$\\beta$是$(p+1)\\times 1$维系数向量，$\\epsilon$是随机扰动，独立同分布于均值为零的正态分布$N(0,\\sigma ^2)$。\n",
    "\n",
    "用分量刻画则是：$y_i = \\beta_0 + \\beta_1\\times x_1 + ... + \\beta_p \\times x_p + \\epsilon_i$，$\\beta_i$是各分量的系数，$\\epsilon_i$是第i个样本的随机扰动。\n",
    "\n",
    "## 二、基本假定\n",
    "\n",
    "1. $r(X)=p+1<n$，$X$是一个满秩矩阵，且列秩线性无关（变量之间不线性相关），行秩大于列秩（样本数大于变量数+1）\n",
    "$x_1, x_2, ..., x_p$是确定变量\n",
    "\n",
    "2. 随机误差项具有零均值和同方差：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    E(\\epsilon_i) = 0 & i = 1,2,...,n\\\\\n",
    "    cov(\\epsilon_i, \\epsilon_j) = \n",
    "    \\begin{cases}\n",
    "        \\sigma^2  & i=j \\\\\n",
    "        0  & i\\neq j\n",
    "    \\end{cases}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "不同样本的随机扰动协方差为零表示不同的样本不相关，在正态假定下即是相互独立\n",
    "\n",
    "3. 正态性假定：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\epsilon_i \\sim N(0, \\sigma^2) & i = 1, 2, ..., n\\\\\n",
    "    \\epsilon_1, \\epsilon_2, ... \\epsilon_n i.i.d\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "由以上假定，结合多元正态分布定义可知\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "E(Y) = X\\beta \\\\\n",
    "var(Y) = \\sigma^2 E \\\\\n",
    "Y \\sim N(X\\beta, \\sigma^2 E)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "## 三、求解\n",
    "\n",
    "回归系数的求解是回归问题的关键，求解系数矩阵有可以推导的**正规方程**（Normal Equation），也可以根据**损失函数**（loss function）通过**梯度下降**（Gradient Descent）等优化方法极小化损失函数求解。\n",
    "\n",
    "**损失函数**是衡量**估计值$\\hat Y$**（回归结果）和真实数据$Y$差距的函数，在回归问题中常用的损失函数是均方误差（MSE, mean square error）:$\\frac 1 2\\sum_{i=1}^n (y_i - \\hat y_i)^2$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    loss &=\\frac 1 2 \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\\\\n",
    "         &=\\frac 1 2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p)^2\n",
    "\\end{split}\n",
    "$$\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\frac {\\partial loss}{\\partial \\beta_0} = -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p) \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_1} = -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p)\\times x_1 \\\\\n",
    "    ... \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_p} = -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p)\\times x_p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "1. 正规方程\n",
    "\n",
    "由于损失是关于系数的开口向上的二次函数，因此它的极小值总是存在的，令上式的各个导数均为零。\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\frac {\\partial loss}{\\partial \\beta_0}|_{\\beta_0 = \\hat \\beta_0} = -\\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1\\times x_1 - ... - \\hat \\beta_p \\times x_p) = 0 \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_1}|_{\\beta_1 = \\hat \\beta_1} = -\\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1\\times x_1 - ... - \\hat \\beta_p \\times x_p)\\times x_1 = 0 \\\\\n",
    "    ... \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_p}|_{\\beta_p = \\hat \\beta_p} = -\\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1\\times x_1 - ... - \\hat \\beta_p \\times x_p)\\times x_p = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "整理成向量形式即是：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "X^T(Y-X\\beta) = 0\\\\\n",
    "X^T Y = X^T X \\beta \\\\\n",
    "\\beta = (X^T X)^{-1}X^T Y\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "2. 梯度下降\n",
    "\n",
    ">梯度的本意是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大(小)值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率（梯度的模）最大。\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla \\beta = \\begin{bmatrix}\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_0}\\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_1}\\\\\n",
    "    ...\\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_p}\n",
    "\\end{bmatrix} &= -X^T(Y-\\hat Y)\\\\\n",
    "Repeat\\ until\\ convergence: \\beta &:= \\beta - step\\times\\nabla\\beta \\\\\n",
    "\\Leftrightarrow Repeat\\ for\\ every\\ j\\ until\\ convergence: \\beta_j &:= \\beta_j + step\\times\\sum_{i=1}^n x_{ij}(y_i - \\hat y_i)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、应用\n",
    "\n",
    "应用数据集是经典的红酒数据集的扩大和更新，因变量是品质（quality），适用于回归和分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-12T08:50:38.992119Z",
     "start_time": "2019-03-12T08:50:36.570780Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2099)\n",
    "\n",
    "white = pd.read_csv('data_set/winequality-white.csv', sep=';')\n",
    "white.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:55:52.891763Z",
     "start_time": "2019-03-09T12:55:52.860767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "x = white.drop(['quality'], axis=1)\n",
    "x.insert(0, 'bias', 1)\n",
    "y = white['quality']\n",
    "\n",
    "n = white.shape[0]\n",
    "p = white.shape[1]\n",
    "\n",
    "x = np.array(x).reshape([n, p])\n",
    "y = np.array(y).reshape([n, 1])\n",
    "\n",
    "index = np.random.permutation(n)\n",
    "\n",
    "n_train = int(0.7*n)\n",
    "n_test = n - n_train\n",
    "\n",
    "train_index = index[0:n_train]\n",
    "test_index = index[n_train:n]\n",
    "\n",
    "train_x = x[train_index,:]\n",
    "train_y = y[train_index,:]\n",
    "\n",
    "test_x = x[test_index,:]\n",
    "test_y = y[test_index,:]\n",
    "\n",
    "print(white.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T13:38:04.984431Z",
     "start_time": "2019-02-22T13:38:04.972918Z"
    }
   },
   "source": [
    "1. 正规方程\n",
    "\n",
    "利用公式直接求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:55:57.166753Z",
     "start_time": "2019-03-09T12:55:56.913792Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.27348050e+02]\n",
      " [ 5.04121231e-02]\n",
      " [-1.85457189e+00]\n",
      " [-4.55813876e-02]\n",
      " [ 7.38987163e-02]\n",
      " [-3.33457213e-01]\n",
      " [ 4.77341398e-03]\n",
      " [-3.73433766e-04]\n",
      " [-1.27415593e+02]\n",
      " [ 6.75370498e-01]\n",
      " [ 5.02043717e-01]\n",
      " [ 2.28705524e-01]]\n"
     ]
    }
   ],
   "source": [
    "def normal_equation(x, y):\n",
    "    computation = np.dot(x.T, x)\n",
    "    computation = np.linalg.inv(computation)\n",
    "    computation = np.dot(computation, x.T)\n",
    "    beta = np.dot(computation, y)\n",
    "    return beta\n",
    "\n",
    "beta_1 = normal_equation(train_x, train_y)\n",
    "print(beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T13:44:14.053332Z",
     "start_time": "2019-02-22T13:44:14.033368Z"
    }
   },
   "source": [
    "计算训练集和测试集损失——均方误差。\n",
    "\n",
    "在处理均方误差时将平方项的和除以样本数进行规范化，这是因为样本越多损失越大，可以大胆假设一下某个样本集合的损失为定值，那么将样本集合的每个样本重复一遍，损失将会变成原来的2倍，尽管本质上两个样本没有显著区别。\n",
    "\n",
    "为了对比回归结果在训练集和测试集上的表现，因此需要对损失函数值除以样本数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:55:59.739353Z",
     "start_time": "2019-03-09T12:55:59.719362Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse(y, y_hat):\n",
    "    diff = y-y_hat\n",
    "    MSE = 0.5*np.dot(diff.T, diff)\n",
    "    return MSE\n",
    "\n",
    "train_y_hat = np.dot(train_x, beta_1)\n",
    "test_y_hat = np.dot(test_x, beta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:00.789190Z",
     "start_time": "2019-03-09T12:56:00.736200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28172218]]\n"
     ]
    }
   ],
   "source": [
    "train_loss = mse(train_y, train_y_hat)\n",
    "print(train_loss/train_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:01.622092Z",
     "start_time": "2019-03-09T12:56:01.613092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28249369]]\n"
     ]
    }
   ],
   "source": [
    "test_loss = mse(test_y, test_y_hat)\n",
    "print(test_loss/test_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，训练集和测试集的损失差别不大，可以认为算法在两个数据集上表现相当，具有良好的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 梯度下降求解\n",
    "\n",
    "梯度下降需要初始化系数矩阵，并在此基础上不断迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:05.375635Z",
     "start_time": "2019-03-09T12:56:05.368633Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize(p):\n",
    "    return np.random.randn(p).reshape([p, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:07.179728Z",
     "start_time": "2019-03-09T12:56:07.127738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.58879755]\n",
      " [ 0.27808812]\n",
      " [ 0.68357992]\n",
      " [-1.12382112]\n",
      " [ 1.29198908]\n",
      " [-0.50970193]\n",
      " [-0.1843707 ]\n",
      " [ 0.68305206]\n",
      " [ 1.00439464]\n",
      " [ 1.86755984]\n",
      " [-1.41410639]\n",
      " [-0.92758716]]\n"
     ]
    }
   ],
   "source": [
    "beta_2 = initialize(p)\n",
    "print(beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降的收敛判断通常是通过损失函数的下降值小于某个阈值$\\delta$，推测这种习惯应该源于数学分析。\n",
    "\n",
    "梯度下降中有两个需要手动设置的超参数，阈值和步长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:44.538658Z",
     "start_time": "2019-03-09T12:56:44.503666Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta, delta=0.0001, step=0.5):\n",
    "    y_hat = np.dot(x, beta)\n",
    "    \n",
    "    loss_aft = mse(y, y_hat)\n",
    "    loss_pre = None\n",
    "\n",
    "    decay = 1\n",
    "\n",
    "    while decay>delta:\n",
    "        loss_pre = loss_aft\n",
    "        \n",
    "        gradient = np.dot(x.T, y_hat - y)/y.shape[0]\n",
    "        beta -= step*gradient\n",
    "        \n",
    "        y_hat = np.dot(x, beta)\n",
    "        loss_aft = mse(y, y_hat)\n",
    "        \n",
    "        decay = loss_pre - loss_aft\n",
    "        \n",
    "        #print(loss_pre)\n",
    "        #print(loss_aft) \n",
    "        #print('decay:'+str(decay))\n",
    "        #print('loss:'+str(loss_aft))\n",
    "        \n",
    "    return beta, loss_aft\n",
    "\n",
    "beta_2, train_loss_2 = gradient_descent(train_x, train_y, beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:48.937978Z",
     "start_time": "2019-03-09T12:56:46.752316Z"
    }
   },
   "outputs": [],
   "source": [
    "beta_2 = initialize(p)\n",
    "beta_2, train_loss_2 = gradient_descent(train_x, train_y, beta_2, \n",
    "                                        delta=0.1, step=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:52.400442Z",
     "start_time": "2019-03-09T12:56:52.389443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66703132]]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss_2/train_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T07:47:27.524366Z",
     "start_time": "2019-02-23T07:47:27.512377Z"
    }
   },
   "source": [
    "可以看出梯度下降的要点在于步长、收敛条件等超参数的设置，步长过长会使得参数变化过大导致损失上升，这在二次函数中十分容易理解，譬如$y=2x^2$，在$y_1 = y(x_1=1)=2$处的梯度为4，步长若设置成1，则$x_2 = x_1 - 4\\times1 = -3$，$y_2 = y(x_2=-3) = 18$，递推下去函数值会越来越大。相应地，如果步长设置为0.5，则$x$将永远在+1，-1之间变动。\n",
    "\n",
    "当我们选取了一个较大的步长时损失可能根本不会下降，当我们选取了一个较大的损失变动阈值和较小的步长时，参数每次变动较小，可能没有训练完全，也即欠拟合。只有步长和阈值均较小时，才能使得参数训练完全，但是同时需要耗费较长时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:57:59.858331Z",
     "start_time": "2019-03-09T12:57:59.845332Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_2(x, y, beta, delta=0.1, step=0.00001):\n",
    "    y_hat = np.dot(x, beta)\n",
    "    \n",
    "    loss_aft = mse(y, y_hat)\n",
    "    loss_pre = None\n",
    "\n",
    "    decay = 1\n",
    "    i=0\n",
    "\n",
    "    while decay>delta:\n",
    "        i+=1\n",
    "        loss_pre = loss_aft\n",
    "        \n",
    "        gradient = np.dot(x.T, y_hat - y)/y.shape[0]\n",
    "        beta -= gradient*(1+1/(2+np.exp(y.shape[0]-i)))*step\n",
    "        \n",
    "        y_hat = np.dot(x, beta)\n",
    "        loss_aft = mse(y, y_hat)\n",
    "        \n",
    "        decay = loss_pre - loss_aft\n",
    "        \n",
    "        #print(i)\n",
    "        #print('decay:'+str(decay))\n",
    "        #print('loss:'+str(loss_aft))\n",
    "        #spoil alert：加了print之后整个文件大小会激增，而且运行时间也会翻倍\n",
    "    \n",
    "    return beta, loss_aft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:04:01.375490Z",
     "start_time": "2019-03-09T13:02:53.362301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\37922\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30090803]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2099)\n",
    "beta_2 = initialize(p)\n",
    "beta_2, train_loss_2 = gradient_descent_2(train_x, train_y, beta_2, \n",
    "                                          delta=0.0001, step=0.00005)\n",
    "print(train_loss_2/train_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T08:35:56.395187Z",
     "start_time": "2019-02-23T08:35:56.383190Z"
    }
   },
   "source": [
    "梯度下降有多种改进方法，主要集中在梯度和步长两个角度，从梯度出发的优化在神经网络中运用较多，比如增加动量（momentum）等，这种优化是为了避免困在局部最优解；由于均方误差函数的性质，回归问题存在唯一的最优解，且前期梯度大，后期梯度小形似一个底部平坦四周陡峭的盆地，因此主要考虑通过增加一个单调递增的函数使步长变大的方法加快后期的训练。\n",
    "\n",
    "这里采用的是类似于logistics回归的函数形式，采用$\\frac {1}{2+e^{-x}}$的形式（分母常数如果是1依然存在梯度过大的问题），函数有上确界1/2使后期步长相对于不加函数之前长了0.5倍。\n",
    "\n",
    "对于多元回归而言求最优解仍然是**建议选用正规方程**求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "\n",
    "[1] 何晓群等 应用回归分析（第三版）[M].北京：中国人民大学出版社，2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
