{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多元回归\n",
    "\n",
    "## 一、概念\n",
    "\n",
    "回归（regression）是一种估计变量关系的统计方法。\n",
    "\n",
    "通过回归可以建立因变量（$Y$）和自变量（$X$）的数量关系，以此进行预测、控制。\n",
    "\n",
    "因变量（$Y$）通常是连续实数，自变量（$X$）可以是连续的也可以是离散的。\n",
    "\n",
    "自变量和因变量之间的关系用矩阵进行刻画：$Y = X\\beta + \\epsilon$，其中$Y$是$n\\times 1$的向量，n代表样本数，$X$是$n\\times (p+1)$维样本矩阵，p是自变量个数，额外一列是偏置项$x_{i0}=1$，$\\beta$是$(p+1)\\times 1$维系数向量，$\\epsilon$是随机扰动，独立同分布于均值为零的正态分布$N(0,\\sigma ^2)$。\n",
    "\n",
    "用分量刻画则是：$y_i = \\beta_0 + \\beta_1\\times x_1 + ... + \\beta_p \\times x_p + \\epsilon_i$，$\\beta_i$是各分量的系数，$\\epsilon_i$是第i个样本的随机扰动。\n",
    "\n",
    "## 二、基本假定\n",
    "\n",
    "1. $r(X)=p+1<n$，$X$是一个满秩矩阵，且列秩线性无关（变量之间不线性相关），行秩大于列秩（样本数大于变量数+1）\n",
    "$x_1, x_2, ..., x_p$是确定变量\n",
    "\n",
    "2. 随机误差项具有零均值和同方差：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    E(\\epsilon_i) = 0 & i = 1,2,...,n\\\\\n",
    "    cov(\\epsilon_i, \\epsilon_j) = \n",
    "    \\begin{cases}\n",
    "        \\sigma^2  & i=j \\\\\n",
    "        0  & i\\neq j\n",
    "    \\end{cases}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "不同样本的随机扰动协方差为零表示不同的样本不相关，在正态假定下即是相互独立\n",
    "\n",
    "3. 正态性假定：\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\epsilon_i \\sim N(0, \\sigma^2) & i = 1, 2, ..., n\\\\\n",
    "    \\epsilon_1, \\epsilon_2, ... \\epsilon_n i.i.d\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "由以上假定，结合多元正态分布定义可知\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    & E(Y) = X\\beta \\\\\n",
    "    & var(Y) = \\sigma^2 E \\\\\n",
    "    & Y \\sim N(X\\beta, \\sigma^2 E)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "## 三、求解\n",
    "\n",
    "回归系数的求解是回归问题的关键，求解系数矩阵有可以推导的**正规方程**（Normal Equation），也可以根据**损失函数**（loss function）通过**梯度下降**（Gradient Descent）等优化方法极小化损失函数求解。\n",
    "\n",
    "**损失函数**是衡量**估计值$\\hat Y$**（回归结果）和真实数据$Y$差距的函数，在回归问题中常用的损失函数是均方误差（MSE, mean square error）:$\\frac 1 2\\sum_{i=1}^n (y_i - \\hat y_i)^2$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    loss &=\\frac 1 2 \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\\\\n",
    "         &=\\frac 1 2 \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p)^2\n",
    "\\end{split}\n",
    "$$\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\frac {\\partial loss}{\\partial \\beta_0} = -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p) \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_1} = -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p)\\times x_1 \\\\\n",
    "    ... \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_p} = -\\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1\\times x_1 - ... - \\beta_p \\times x_p)\\times x_p\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "1. 正规方程\n",
    "\n",
    "由于损失是关于系数的开口向上的二次函数，因此它的极小值总是存在的，令上式的各个导数均为零。\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\frac {\\partial loss}{\\partial \\beta_0}|_{\\beta_0 = \\hat \\beta_0} = -\\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1\\times x_1 - ... - \\hat \\beta_p \\times x_p) = 0 \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_1}|_{\\beta_1 = \\hat \\beta_1} = -\\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1\\times x_1 - ... - \\hat \\beta_p \\times x_p)\\times x_1 = 0 \\\\\n",
    "    ... \\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_p}|_{\\beta_p = \\hat \\beta_p} = -\\sum_{i=1}^n (y_i - \\hat \\beta_0 - \\hat \\beta_1\\times x_1 - ... - \\hat \\beta_p \\times x_p)\\times x_p = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "整理成向量形式即是：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "X^T(Y-X\\beta) = 0\\\\\n",
    "X^T Y = X^T X \\beta \\\\\n",
    "\\beta = (X^T X)^{-1}X^T Y\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "2. 梯度下降\n",
    "\n",
    ">梯度的本意是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大(小)值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率（梯度的模）最大。\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla \\beta = \\begin{bmatrix}\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_0}\\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_1}\\\\\n",
    "    ...\\\\\n",
    "    \\frac {\\partial loss}{\\partial \\beta_p}\n",
    "\\end{bmatrix} &= -X^T(Y-\\hat Y)\\\\\n",
    "Repeat\\ until\\ convergence: \\beta &:= \\beta - step\\times\\nabla\\beta \\\\\n",
    "\\Leftrightarrow Repeat\\ for\\ every\\ j\\ until\\ convergence: \\beta_j &:= \\beta_j + step\\times\\sum_{i=1}^n x_{ij}(y_i - \\hat y_i)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、应用\n",
    "\n",
    "应用数据集是经典的红酒数据集的扩大和更新，因变量是品质（quality），适用于回归和分类问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T14:46:06.851575Z",
     "start_time": "2019-03-17T14:46:06.741615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "1            6.3              0.30         0.34             1.6      0.049   \n",
       "2            8.1              0.28         0.40             6.9      0.050   \n",
       "3            7.2              0.23         0.32             8.5      0.058   \n",
       "4            7.2              0.23         0.32             8.5      0.058   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
       "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
       "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
       "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      8.8        6  \n",
       "1      9.5        6  \n",
       "2     10.1        6  \n",
       "3      9.9        6  \n",
       "4      9.9        6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(2099)\n",
    "\n",
    "white = pd.read_csv('data_set/winequality-white.csv', sep=';')\n",
    "white.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T14:46:12.680664Z",
     "start_time": "2019-03-17T14:46:12.650670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4898, 12)\n"
     ]
    }
   ],
   "source": [
    "x = white.drop(['quality'], axis=1)\n",
    "x.insert(0, 'bias', 1)\n",
    "y = white['quality']\n",
    "\n",
    "n = white.shape[0]\n",
    "p = white.shape[1]\n",
    "\n",
    "x = np.array(x).reshape([n, p])\n",
    "y = np.array(y).reshape([n, 1])\n",
    "\n",
    "np.random.seed(99)\n",
    "index = np.random.permutation(n)\n",
    "\n",
    "n_train = int(0.7*n)\n",
    "n_test = n - n_train\n",
    "\n",
    "train_index = index[0:n_train]\n",
    "test_index = index[n_train:n]\n",
    "\n",
    "train_x = x[train_index,:]\n",
    "train_y = y[train_index,:]\n",
    "\n",
    "test_x = x[test_index,:]\n",
    "test_y = y[test_index,:]\n",
    "\n",
    "print(white.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T13:38:04.984431Z",
     "start_time": "2019-02-22T13:38:04.972918Z"
    }
   },
   "source": [
    "1. 正规方程\n",
    "\n",
    "利用公式直接求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T14:46:29.630628Z",
     "start_time": "2019-03-17T14:46:29.611631Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.41258269e+02]\n",
      " [ 1.31908754e-01]\n",
      " [-1.81022513e+00]\n",
      " [ 9.58902831e-02]\n",
      " [ 1.10948761e-01]\n",
      " [ 3.05027005e-01]\n",
      " [ 3.41232024e-03]\n",
      " [-8.14510194e-05]\n",
      " [-2.42591122e+02]\n",
      " [ 1.03315525e+00]\n",
      " [ 6.72193199e-01]\n",
      " [ 8.30258911e-02]]\n"
     ]
    }
   ],
   "source": [
    "def normal_equation(x, y):\n",
    "    computation = np.dot(x.T, x)\n",
    "    computation = np.linalg.inv(computation)\n",
    "    computation = np.dot(computation, x.T)\n",
    "    beta = np.dot(computation, y)\n",
    "    return beta\n",
    "\n",
    "beta_1 = normal_equation(train_x, train_y)\n",
    "print(beta_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T13:44:14.053332Z",
     "start_time": "2019-02-22T13:44:14.033368Z"
    }
   },
   "source": [
    "计算训练集和测试集损失——均方误差。\n",
    "\n",
    "在处理均方误差时将平方项的和除以样本数进行规范化，这是因为样本越多损失越大，可以大胆假设一下某个样本集合的损失为定值，那么将样本集合的每个样本重复一遍，损失将会变成原来的2倍，尽管本质上两个样本没有显著区别。\n",
    "\n",
    "为了对比回归结果在训练集和测试集上的表现，因此需要对损失函数值除以样本数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:01:40.078722Z",
     "start_time": "2019-03-17T15:01:39.925746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2767015]]\n"
     ]
    }
   ],
   "source": [
    "def mse(y, y_hat):\n",
    "    diff = y-y_hat\n",
    "    MSE = 0.5*np.dot(diff.T, diff)\n",
    "    return MSE\n",
    "\n",
    "train_y_hat = np.dot(train_x, beta_1)\n",
    "test_y_hat = np.dot(test_x, beta_1)\n",
    "\n",
    "train_loss = mse(train_y, train_y_hat)\n",
    "print(train_loss/train_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:01.622092Z",
     "start_time": "2019-03-09T12:56:01.613092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28249369]]\n"
     ]
    }
   ],
   "source": [
    "test_loss = mse(test_y, test_y_hat)\n",
    "print(test_loss/test_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，训练集和测试集的损失差别不大，可以认为算法在两个数据集上表现相当，具有良好的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 梯度下降求解\n",
    "\n",
    "梯度下降需要初始化系数矩阵，并在此基础上不断迭代。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:05.375635Z",
     "start_time": "2019-03-09T12:56:05.368633Z"
    }
   },
   "outputs": [],
   "source": [
    "def initialize(p):\n",
    "    return np.random.randn(p).reshape([p, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:07.179728Z",
     "start_time": "2019-03-09T12:56:07.127738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.58879755]\n",
      " [ 0.27808812]\n",
      " [ 0.68357992]\n",
      " [-1.12382112]\n",
      " [ 1.29198908]\n",
      " [-0.50970193]\n",
      " [-0.1843707 ]\n",
      " [ 0.68305206]\n",
      " [ 1.00439464]\n",
      " [ 1.86755984]\n",
      " [-1.41410639]\n",
      " [-0.92758716]]\n"
     ]
    }
   ],
   "source": [
    "beta_2 = initialize(p)\n",
    "print(beta_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降的收敛判断通常是通过损失函数的下降值小于某个阈值$\\delta$，推测这种习惯应该源于数学分析。\n",
    "\n",
    "梯度下降中有两个需要手动设置的超参数，阈值和步长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:44.538658Z",
     "start_time": "2019-03-09T12:56:44.503666Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, beta, delta=0.0001, step=0.5):\n",
    "    y_hat = np.dot(x, beta)\n",
    "    \n",
    "    loss_aft = mse(y, y_hat)\n",
    "    loss_pre = None\n",
    "\n",
    "    decay = 1\n",
    "\n",
    "    while decay>delta:\n",
    "        loss_pre = loss_aft\n",
    "        \n",
    "        gradient = np.dot(x.T, y_hat - y)/y.shape[0]\n",
    "        beta -= step*gradient\n",
    "        \n",
    "        y_hat = np.dot(x, beta)\n",
    "        loss_aft = mse(y, y_hat)\n",
    "        \n",
    "        decay = loss_pre - loss_aft\n",
    "        \n",
    "        #print(loss_pre)\n",
    "        #print(loss_aft) \n",
    "        #print('decay:'+str(decay))\n",
    "        #print('loss:'+str(loss_aft))\n",
    "        \n",
    "    return beta, loss_aft\n",
    "\n",
    "beta_2, train_loss_2 = gradient_descent(train_x, train_y, beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:48.937978Z",
     "start_time": "2019-03-09T12:56:46.752316Z"
    }
   },
   "outputs": [],
   "source": [
    "beta_2 = initialize(p)\n",
    "beta_2, train_loss_2 = gradient_descent(train_x, train_y, beta_2, \n",
    "                                        delta=0.1, step=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T12:56:52.400442Z",
     "start_time": "2019-03-09T12:56:52.389443Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66703132]]\n"
     ]
    }
   ],
   "source": [
    "print(train_loss_2/train_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T07:47:27.524366Z",
     "start_time": "2019-02-23T07:47:27.512377Z"
    }
   },
   "source": [
    "可以看出梯度下降的要点在于步长、收敛条件等超参数的设置，步长过长会使得参数变化过大导致损失上升，这在二次函数中十分容易理解，譬如$y=2x^2$，在$y_1 = y(x_1=1)=2$处的梯度为4，步长若设置成1，则$x_2 = x_1 - 4\\times1 = -3$，$y_2 = y(x_2=-3) = 18$，递推下去函数值会越来越大。相应地，如果步长设置为0.5，则$x$将永远在+1，-1之间变动。\n",
    "\n",
    "当我们选取了一个较大的步长时损失可能根本不会下降，当我们选取了一个较大的损失变动阈值和较小的步长时，参数每次变动较小，可能没有训练完全，也即欠拟合。只有步长和阈值均较小时，才能使得参数训练完全，但是同时需要耗费较长时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-16T15:37:54.444701Z",
     "start_time": "2019-03-16T15:37:54.428705Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent_2(x, y, beta, delta=0.1, step=0.00001):\n",
    "    y_hat = np.dot(x, beta)\n",
    "    \n",
    "    loss_aft = mse(y, y_hat)\n",
    "    loss_pre = None\n",
    "\n",
    "    decay = 1\n",
    "    i=0\n",
    "\n",
    "    while decay>delta:\n",
    "        i+=1\n",
    "        loss_pre = loss_aft\n",
    "        \n",
    "        gradient = np.dot(x.T, y_hat - y)/y.shape[0]\n",
    "        beta -= gradient*(1+1/(2+np.exp(y.shape[0]-i)))*step\n",
    "        \n",
    "        y_hat = np.dot(x, beta)\n",
    "        loss_aft = mse(y, y_hat)\n",
    "        \n",
    "        decay = loss_pre - loss_aft\n",
    "        \n",
    "        #print(i)\n",
    "        #print('decay:'+str(decay))\n",
    "        #print('loss:'+str(loss_aft))\n",
    "        #spoil alert：加了print之后整个文件大小会激增，而且运行时间也会翻倍\n",
    "    \n",
    "    return beta, loss_aft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T13:04:01.375490Z",
     "start_time": "2019-03-09T13:02:53.362301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\37922\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30090803]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2099)\n",
    "beta_2 = initialize(p)\n",
    "beta_2, train_loss_2 = gradient_descent_2(train_x, train_y, beta_2, \n",
    "                                          delta=0.0001, step=0.00005)\n",
    "print(train_loss_2/train_y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-23T08:35:56.395187Z",
     "start_time": "2019-02-23T08:35:56.383190Z"
    }
   },
   "source": [
    "梯度下降有多种改进方法，主要集中在梯度和步长两个角度，从梯度出发的优化在神经网络中运用较多，比如增加动量（momentum）等，这种优化是为了避免困在局部最优解；由于均方误差函数的性质，回归问题存在唯一的最优解，且前期梯度大，后期梯度小形似一个底部平坦四周陡峭的盆地，因此主要考虑通过增加一个单调递增的函数使步长变大的方法加快后期的训练。\n",
    "\n",
    "这里采用的是类似于logistics回归的函数形式，采用$\\frac {1}{2+e^{-x}}$的形式（分母常数如果是1依然存在梯度过大的问题），函数有上确界1/2使后期步长相对于不加函数之前长了0.5倍。\n",
    "\n",
    "对于多元回归而言求最优解仍然是**建议选用正规方程**求解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、回归方程的显著性检验\n",
    "\n",
    "线性方程是统计学中最精华方法最多的部分，检验回归方程的显著性更多是统计学方面的内容，在机器学习领域体现较少，但作为回归不可分割的一部分，仍然值得介绍。\n",
    "\n",
    "对多元线性回归的检验主要有两种：一是针对回归方程**整体是否显著**的**F检验**、二是回归**系数是否显著**的**t检验**。\n",
    "\n",
    "### 1. F检验\n",
    "\n",
    "#### 1）抽样分布-F分布\n",
    "\n",
    "F分布定义为两个卡方分布除以自由度的比值：\n",
    "\n",
    "$$\n",
    "F = \\frac{\\chi_1^2/m}{\\chi_2^2/n}\\sim F(m,n)\n",
    "$$\n",
    "\n",
    "其中卡方分布是多个独立标准正态分布的平方和，其自由度就是正态分布的个数：\n",
    "\n",
    "$$\n",
    "\\chi^2(n) = X_1^2 + X_2^2 + \\dots + X_n^2,\\ \\ X_i\\ i.i.d\\ N(0, 1)\n",
    "$$\n",
    "\n",
    "#### 2）平方和分解\n",
    "\n",
    "回忆方差的表达式：\n",
    "\n",
    "$$\n",
    "MSE = \\frac1{n-1}\\sum_{i=1}^n(y_i-\\bar y_i)^2\n",
    "$$\n",
    "\n",
    "我们取求和部分进行分解：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\sum_{i=1}^n(y_i-\\bar y)^2 &= \\sum_{i=1}^n(y_i - \\hat y_i + \\hat y_i -\\bar y)\\\\\n",
    "        &= \\sum_{i=1}^n(y_i - \\hat y_i)^2 + \\sum_{i=1}^n(\\hat y_i -\\bar y)^2 + \\sum_{i=1}^n2(y_i - \\hat y_i)(\\hat y_i -\\bar y)\\\\\n",
    "        &= \\sum_{i=1}^n(y_i - \\hat y_i)^2 + \\sum_{i=1}^n(\\hat y_i -\\bar y)^2\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "其中，\n",
    "- $\\sum_{i=1}^n(y_i-\\bar y)^2$称为总平方和，SST（Sum of Square for Total），\n",
    "\n",
    "- $\\sum_{i=1}^n(\\hat y_i -\\bar y)^2$称为回归平方和，SSR（Sum of Square for Regression），\n",
    "\n",
    "- $\\sum_{i=1}^n(y_i - \\hat y_i)^2$称为残差平方和，SSE（Sum of Square for Error），\n",
    "\n",
    "第二行到第三行等式成立的条件是（5）方程组。\n",
    "\n",
    "\n",
    "#### 3）\n",
    "\n",
    "对多元回归模型的显著性检验就是看各个变量$x_1, x_2,...,x_p$整体上是否对随机变量y具有明显的影响，因此检验问题的原假设为：\n",
    "\n",
    "$$\n",
    "H_0:\\ \\beta_1=\\beta_2=\\dots=\\beta_p=0\n",
    "$$\n",
    "\n",
    "备择假设是其否命题，即系数不全为0。\n",
    "\n",
    "构造F统计量：\n",
    "\n",
    "$$\n",
    "F=\\frac{SSR/p}{SSE/(n-p-1)}\n",
    "$$\n",
    "\n",
    "F统计量服从F（p, n-p-1），确定显著性水平后查表即可判断模型是否显著"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. t检验\n",
    "\n",
    "#### 1）抽样分布-t分布\n",
    "\n",
    "设随机变量$X_1$，$X_2$独立，且$X_1\\sim N(0,1)$，$X_2\\sim \\chi^2(n)$，\n",
    "\n",
    "$$\n",
    "t=\\frac{X_1}{\\sqrt{X_2/n}}\n",
    "$$\n",
    "\n",
    "为自由度为n的t分布，$t\\sim t(n)$\n",
    "\n",
    "#### 2）假设检验\n",
    "\n",
    "回归系数的显著性检验是针对单个系数，检验这个变量是否对随机变量y有影响，原备择假设为：\n",
    "\n",
    "$$\n",
    "H_0:\\beta_i = 0,\\ \\ \\ H_1:\\beta_i \\neq 0\n",
    "$$\n",
    "\n",
    "如果接受原假设则变量$x_i$对y的影响不显著，拒绝原则设则是有显著影响。\n",
    "\n",
    "回归方程的系数向量满足多元正态分布：\n",
    "\n",
    "$$\n",
    "\\beta \\sim N(\\beta,\\sigma^2(X^TX)^{-1})\n",
    "$$\n",
    "\n",
    "记\n",
    "\n",
    "$$\n",
    "(X^TX)^{-1}=(c_{ij}), i,j=1,\\dots,p\n",
    "$$\n",
    "\n",
    "于是有\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    &E(\\hat \\beta_j) = \\beta_j, Var(\\hat \\beta_j)=c_{jj}\\sigma^2\\\\\n",
    "    &\\hat \\beta_j\\sim N(\\beta_j, c_{jj}\\sigma^2),j=1,\\dots,p\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "据此可以构造t统计量\n",
    "\n",
    "$$\n",
    "t_j = \\frac{\\hat \\beta_j}{\\sqrt c_{jj}\\hat\\sigma}\n",
    "$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\hat \\sigma = \\sqrt{\\frac{1}{n-p-1} \\sum_{i=1}^n e_i^2} = \\sqrt{\\frac{1}{n-p-1} \\sum_{i=1}^n (y_i - \\hat y_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 拟合优度\n",
    "\n",
    "拟合优度是衡量回归方程对样本观测程度的量，它的定义很好理解，即是模型可解释的偏差比上数据总的偏差。这里的偏差不是模型偏差-方差权衡（Bias-Variance Tradeoff）的偏差，而是指预测值和观测值远离均值的程度。\n",
    "\n",
    "也有将拟合优度称为样本决定系数的说法，这也反映了拟合优度是样本各个变量能决定的（能从样本的变量中学习到的）信息占总信息的比值;\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{SSR}{SST} = 1- \\frac{SSE}{SST}\n",
    "$$\n",
    "\n",
    "在实际情况中R方能达到0.7已经属于拟合程度很好的情况，一些过高的R方反而不能说明模型很好，倒【有可能】说明数据是编造得到的。\n",
    "\n",
    "我们还没有讨论到正则化，事实上增加变量几乎总是会改善模型的表现，或者说提升R方的值。正如我们先前做的t检验，一些变量或许会提升表现，但是可能是不显著的，因此我们不能断言变量越多模型就越好，在未知的数据上多余的变量反而可能会拖后腿，因此有了调整后的R方（Adjusted R-square），能对非显著的变量进行惩罚。\n",
    "\n",
    "$$\n",
    "Adjusted-R^2 = 1-\\frac{(1-R^2)(n-1)}{n-p-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:06:46.797567Z",
     "start_time": "2019-03-17T15:06:46.783573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1392.96660424]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f_test(y, y_hat):\n",
    "    dim = y.shape\n",
    "    dim_hat = y_hat.shape\n",
    "    if dim == dim_hat:\n",
    "        ssr = np.var(y_hat)*dim[0]\n",
    "        sse = np.dot((y-y_hat).T, y-y_hat)\n",
    "        f = (ssr/dim[1])/(sse/(dim[0]-dim[1]-1))\n",
    "        return f\n",
    "    else:\n",
    "        print(\"Dimension not match\")\n",
    "\n",
    "f_test(train_y, train_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:16:20.912027Z",
     "start_time": "2019-03-17T15:16:20.889033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.42875825e-01],\n",
       "       [ 1.34259783e+02],\n",
       "       [-7.36466390e+01],\n",
       "       [ 5.28961961e+00],\n",
       "       [ 7.90648232e+02],\n",
       "       [ 5.00916520e-01],\n",
       "       [ 2.59578235e+03],\n",
       "       [-2.97790532e+02],\n",
       "       [-2.38010893e-01],\n",
       "       [ 4.41834376e+01],\n",
       "       [ 3.40475709e+01],\n",
       "       [ 5.17330563e+01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def t_test(x, y, beta, y_hat):\n",
    "    n, p = x.shape\n",
    "    c = np.linalg.inv(np.dot(x.T, x))\n",
    "    c = np.diag(c).reshape(p, 1)\n",
    "    sse = np.dot((y-y_hat).T, y-y_hat)\n",
    "    sigma = np.sqrt(sse/(n-p-1))\n",
    "    t = beta/c/sigma\n",
    "    return t\n",
    "\n",
    "t_test(train_x, train_y, beta_1, train_y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T15:19:41.447942Z",
     "start_time": "2019-03-17T15:19:41.431943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.40658687]]), array([[0.40641366]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def adjusted_r2(y, y_hat):\n",
    "    n, p = y.shape\n",
    "    ssr = np.var(y_hat)*n\n",
    "    sst = np.dot((y-y_hat).T, y-y_hat)\n",
    "    r2 = ssr/sst\n",
    "    adj_r2 = 1-(1-r2)*(n-1)/(n-p-1)\n",
    "    return r2, adj_r2\n",
    "\n",
    "adjusted_r2(train_y, train_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考资料：\n",
    "\n",
    "[1] 何晓群等 应用回归分析（第三版）[M].北京：中国人民大学出版社，2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
