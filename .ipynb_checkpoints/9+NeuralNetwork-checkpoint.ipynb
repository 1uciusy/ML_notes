{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络\n",
    "\n",
    "## 一、概念\n",
    "\n",
    "神经网络是时下最热的人工智能话题，而神经网络的历史也由来已久，近年来的算力大爆发使人工智能和神经网络发现了彼此。\n",
    "\n",
    "神经网络通过神经元进行组织，数据从上一层神经元流向下一层神经元直到输出神经元，损失函数衡量预测和输出之间的差距，再通过反向传播更新各层神经元的参数。\n",
    "\n",
    "神经网络由如下元素构成：\n",
    "\n",
    "1. 输入层：数据从输入层进入模型\n",
    "\n",
    "\n",
    "2. 隐藏层：数据在隐藏层中进行交互和组合\n",
    "\n",
    "\n",
    "3. 输出层：输出层输出预测结果\n",
    "\n",
    "\n",
    "4. 激活函数：各个神经元的对上一层的输入进行非线性处理的函数\n",
    "\n",
    "\n",
    "5. 损失函数：衡量预测结果和实际结果的差距\n",
    "\n",
    "\n",
    "6. 优化器：即以何种方式更新参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、符号说明\n",
    "\n",
    "- $X$：输入数据，$X\\in R^{p\\times n}$，p代表变量数，n代表样本数\n",
    "\n",
    "\n",
    "- $\\hat Y$：输出数据，$\\hat Y\\in R^{k\\times n}$，n代表样本数，多分类时k代表分类数，二分类和回归时k为1\n",
    "\n",
    "\n",
    "- $Y$：实际结果，$Y\\in R^{k\\times n}$，n代表样本数，多分类时k代表分类数，二分类和回归时k为1\n",
    "\n",
    "\n",
    "- $p_i$：第i层的神经元数，$p_0=p$\n",
    "\n",
    "\n",
    "- $W_i$：从第i-1层向第i层传播的矩阵，$W_i\\in R^{p_{i}\\times p_{i-1}}$，输入层为第0层时，$W_1\\in R^{p_{1}\\times p}$\n",
    "\n",
    "\n",
    "- $\\alpha(z)$：激活函数，每一层每一个神经元的激活函数都可以不同，此处统一用α\n",
    "\n",
    "\n",
    "- $g(z)$：输出层的激活函数，通常和隐藏层的激活函数不同\n",
    "\n",
    "\n",
    "- $b_i$：第i层的偏置项，$b_i \\in R^{p_{i+1}}$\n",
    "\n",
    "\n",
    "- $Z_i$：上一层激活函数的线性组合，$Z_i \\in R^{p_i\\times n}$\n",
    "\n",
    "\n",
    "- $A_i$：线性组合的激活函数值，$A_i \\in R^{p_i\\times n}$\n",
    "\n",
    "\n",
    "- \\* ：逐元素相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、Feed Forward 前向传播\n",
    "\n",
    "### 1.从输入层到第一个隐藏层\n",
    "\n",
    "首先是对输入数据的线性组合，由于偏执项是一个向量，对所有n个数据来说都相等。虽然此处维度按照线性代数并不能严格成立（因为$W_1X\\in R^{p_1\\times n}$，$b_1 \\in R^{p_1\\times 1}$），但是由于numpy中的广播（broadcast）机制存在，在编程中以下公式是成立的。如果非要按照数学定义上成立可以对$b_1$乘上一个$1\\times n$的值全为1的向量。\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "        &Z_1 = W_1X + b_1 \\ \\ \\in R^{p_1\\times n}\\\\\n",
    "    \\Leftrightarrow & Z_1 = W_1X + b_11^{1\\times n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "然后是对第一层的各个神经元进行“激活”，对线性组合进行逐元素的函数计算\n",
    "\n",
    "$$\n",
    "A_1 = \\alpha(Z_1)\\ \\ \\in R^{p_1\\times n}\n",
    "$$\n",
    "\n",
    "### 2.从第i-1层到第i层\n",
    "\n",
    "与输入不同，此时是将上一层的激活函数值进行线性组合：\n",
    "\n",
    "$$\n",
    "Z_i = W_iA_{i-1} + b_i \\ \\ \\in R^{p_i\\times n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A_i = \\alpha(Z_i)\\ \\ \\in R^{p_i\\times n}\n",
    "$$\n",
    "\n",
    "### 3.从最后一个隐藏层到输出层\n",
    "\n",
    "假设输入层是第0层，第1——m-1层是隐藏层，第m层是输出层。如果是二分类、回归等情况，则输出层只有一个神经元，若是多分类等情况则有多个神经元，将在后面介绍，暂时假定只有一个输出：\n",
    "\n",
    "$$\n",
    "Z_m = W_m A_{m-1} + b_m \\ \\ \\in R^{k\\times n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat Y = A_m = g(Z_m)\\ \\ \\in R^{k\\times n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、激活函数\n",
    "\n",
    "激活函数有多种多样，本质上都是为了进行非线性组合，还有易于进行求导运算以便更新参数。此处简单介绍几种激活函数\n",
    "\n",
    "### 1.sigmoid函数\n",
    "\n",
    "Sigmoid函数已经在logistic回归中介绍过：\n",
    "\n",
    "$$\n",
    "sigmoid(z)=\\frac{1}{1+1^{-z}}\n",
    "$$\n",
    "\n",
    "它是一种较早期的激活函数，现在多用于最后输出层的激活而不用在隐藏层中，这是因为当x远离原点时它的梯度会非常接近0，会造成非常著名的“梯度消失”的现象。\n",
    "\n",
    "考虑sigmoid函数的导数：\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz} sigmoid(z)=\\frac{e^{-z}}{(1+1^{-z})^2}\n",
    "$$\n",
    "\n",
    "当z=0时其梯度最大为0.25，当神经网络的层数变深时便是指数倍地降低，这便是“梯度消失”最直观和简洁的解释。\n",
    "\n",
    "### 2.Relu（Rectified Linear Unit, 线性整流函数）\n",
    "\n",
    "Relu也曾是红极一时的激活函数，因其简洁的函数形式和导数形式（x大于零导数为1，其他情况为0）使计算成本大大降低，但同时这也带来了神经元没有被激活的情况。这是因为当输入小于0时，输出和梯度都为0，导致神经元“死亡”。\n",
    "\n",
    "$$\n",
    "Relu(z) = max(0, z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz}Relu(z) = \\begin{cases}\n",
    "1 & z>0 \\\\\n",
    "0 & z\\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 3.leaky Relu\n",
    "\n",
    "leaky Relu是我最喜欢的激活函数，因为它兼具了Relu的优点，且当输入小于零时不会出现神经元死亡的情况，k通常的设置为0.1。\n",
    "\n",
    "$$\n",
    "leakyRelu(z, k) = max(kz, z)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d}{dz}leakyRelu(z) = \\begin{cases}\n",
    "1 & z>0 \\\\\n",
    "k & z\\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### 4.softmax\n",
    "\n",
    "softmax是专门用于多分类的输出层的激活函数，有两种等价形式，一种是针对K类有K个输出的线性相关的形式（即下式），另一个是针对K类有K-1个输出的线性无关的形式。\n",
    "\n",
    "$$\n",
    "softmax(z) = \\begin{bmatrix}\n",
    "    \\frac{e^{z_1}}{\\sum_{i=1}^ke^{z_i}}\\\\\n",
    "    \\frac{e^{z_2}}{\\sum_{i=1}^ke^{z_i}}\\\\\n",
    "    ...\\\\\n",
    "    \\frac{e^{z_j}}{\\sum_{i=1}^ke^{z_i}}\\\\\n",
    "    ...\\\\\n",
    "    \\frac{e^{z_k}}{\\sum_{i=1}^ke^{z_i}}\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\hat y_1\\\\\n",
    "    \\hat y_2\\\\\n",
    "    ...\\\\\n",
    "    \\hat y_i\\\\\n",
    "    ...\\\\\n",
    "    \\hat y_k\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "它的针对单一分量的偏导数形式和sigmoid函数极为相似：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\partial}{\\partial z_i}softmax(z) &= \\frac{d}{dz_i} \\frac{e^{z_i}}{a+e^{z_i}}= \\frac{ae^{z_i}}{(a+e^{z_i})^2} \\\\\n",
    "        &= \\frac{ae^{z_i}+a^2-a^2}{(a+e^{z_i})^2} \\\\\n",
    "        &= \\frac{a(e^{z_i}+a)-a^2}{(a+e^{z_i})^2} \\\\\n",
    "        &= \\frac{a}{a+e^{z_i}} - \\left(\\frac{a}{a+e^{z_i}}\\right)^2 \\\\\n",
    "        &= \\frac{a}{a+e^{z_i}}\\left(1-\\frac{a}{a+e^{z_i}}\\right) \\\\\n",
    "        &= \\left(1-\\frac{e^{z_i}}{a+e^{z_i}}\\right)\\frac{e^{z_i}}{a+e^{z_i}}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "则它的梯度为：\n",
    "\n",
    "$$\n",
    "\\triangledown softmax(z)=\\begin{bmatrix}\n",
    "    \\hat y_1(1-\\hat y_1)\\\\\n",
    "    \\hat y_2(1-\\hat y_2)\\\\\n",
    "    ...\\\\\n",
    "    \\hat y_i(1-\\hat y_i)\\\\\n",
    "    ...\\\\\n",
    "    \\hat y_k(1-\\hat y_k)\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、损失函数\n",
    "\n",
    "二分类和回归的损失函数不再赘述，和logistic回归和多元线性回归类似，这里介绍多分类的损失函数。\n",
    "\n",
    "多分类的损失函数和二分类相同，也是通过似然函数进行定义：假设随机变量Y一共有K个取值，第i个样本对第j个取值的概率估计值为：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "P(y_i=j) = \\hat y_{ij} \\ \\ j=1,2,...,k \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "则对n个样本，其似然函数为：\n",
    "\n",
    "$$\n",
    "likelihood(Y, \\hat Y)=\\prod_{i=1}^n\\prod_{j=1}^k \\hat y_{ij}^{I(y_i=j)}\n",
    "$$\n",
    "\n",
    "对其求自然对数，除以样本数进行标准化取负数：\n",
    "\n",
    "$$\n",
    "loss(Y, \\hat Y) = -\\frac1n\\sum_{i=1}^n \\sum_{j=1}^k I(y_i=j)ln(\\hat y_{ij})\n",
    "$$\n",
    "\n",
    "这就是最终的损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、Backward propagation 反向传播\n",
    "\n",
    "反向传播是神经网络更新参数最经典也是最有效、最具有广泛性的算法。\n",
    "\n",
    "反向传播的基础仍然是梯度下降法。\n",
    "\n",
    "### 1.输出层到第一个隐藏层的反向传播\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial y_ij}loss = \\frac1n\\frac{I(y_i=j)}{\\hat y_{ij}}\n",
    "$$\n",
    "\n",
    "由于输出是$\\hat Y \\in R^{k\\times n}$向量，损失对输出层的梯度和输出保持一致的维度：\n",
    "\n",
    "$$\n",
    "\\frac{\\triangledown loss}{\\triangledown \\hat Y}=\n",
    "\\frac1n\\begin{bmatrix}\n",
    "    \\frac{I(y_1=1)}{\\hat y_{11}} & \\dots & \\frac{I(y_1=k)}{\\hat y_{1k}}\\\\\n",
    "     & \\ddots & \\vdots \\\\\n",
    "    \\frac{I(y_n=1)}{\\hat y_{n1}} & & \\frac{I(y_n=k)}{\\hat y_{nk}}\n",
    "\\end{bmatrix}^T \\in R^{k\\times n}\n",
    "$$\n",
    "\n",
    "对输出层的激活函数有$\\hat Y = g(Z_m)=softmax(Z_m)\\in R^{k\\times n}$，$Z_m\\in R^{k\\times n}$\n",
    "\n",
    "$$\n",
    "\\frac{\\triangledown\\hat Y}{\\triangledown Z_m} = \n",
    "\\begin{bmatrix}\n",
    "    \\hat y_{11}(1-\\hat y_{11}) & \\dots & \\hat y_{1k}(1-\\hat y_{1k})\\\\\n",
    "    & \\ddots & \\vdots \\\\\n",
    "    \\hat y_{n1}(1-\\hat y_{n1}) &  & \\hat y_{nk}(1-\\hat y_{nk})\n",
    "\\end{bmatrix}^T \\in R^{k\\times n}\n",
    "$$\n",
    "\n",
    "此时还未涉及到参数的更新，而$Z_m = W_mA_{m-1} + b_m$中$W_m\\in R^{p_m=k\\times p_{m-1}}$、$b_m\\in R^{k\\times 1}$、$A_{m-1}\\in R^{p_{m-1}\\times n}$均为参数，其中前两个好理解，而激活函数值也需要更新是因为它是先前输入的函数，需要通过对激活函数更新使梯度传导到更靠前的隐藏层。\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    &\\frac{\\triangledown Z_m}{\\triangledown W_m} = A_{m-1}\\in R^{p_{m-1}\\times n}\\\\\n",
    "    \\\\\n",
    "    &\\frac{\\triangledown Z_m}{\\triangledown A_{m-1}} = W_m^T\\in R^{p_{m-1}\\times k}\\\\\n",
    "    \\\\\n",
    "    &\\frac{\\triangledown Z_m}{\\triangledown b_m} = 1^{1\\times n} \\in R^{1\\times n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "将其和之前的梯度结合起来：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\triangledown loss}{\\triangledown W_m} &= \\left(\\frac{\\triangledown loss}{\\triangledown \\hat Y}*\\frac{\\triangledown\\hat Y}{\\triangledown Z_m}\\right)\\left(\\frac{\\triangledown Z_m}{\\triangledown W_m}\\right)^T \\in R^{p_m=k\\times p_{m-1}}\\\\\n",
    "    \\frac{\\triangledown loss}{\\triangledown A_{m-1}} &= \\frac{\\triangledown Z_m}{\\triangledown A_{m-1}}\\left(\\frac{\\triangledown loss}{\\triangledown \\hat Y}*\\frac{\\triangledown\\hat Y}{\\triangledown Z_m}\\right)\\in R^{p_{m-1}\\times n}\\\\\n",
    "    \\frac{\\triangledown loss}{\\triangledown b_m} &= \\left(\\frac{\\triangledown loss}{\\triangledown \\hat Y}*\\frac{\\triangledown\\hat Y}{\\triangledown Z_m}\\right)1^{n\\times 1}\\in R^{k\\times 1}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.第i层到第i-1层的反向传播\n",
    "\n",
    "从第i层到第i-1层的反向传播和从输出层到最后一个隐藏层的推导相似：\n",
    "\n",
    "假设$\\frac{\\triangledown loss}{\\triangledown A_i}\\in R^{p_i\\times n}$已知，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    A_i &= \\begin{bmatrix}\n",
    "            \\alpha (z_{11}) & \\dots & \\alpha (z_{1p_i})\\\\\n",
    "             & \\ddots & \\vdots \\\\\n",
    "            \\alpha (z_{n1}) & &\\alpha(z_{np_i}) \n",
    "          \\end{bmatrix}\\in R^{p_i\\times n}\\\\\n",
    "       \\frac{\\triangledown A_i}{\\triangledown Z_i}&=\\begin{bmatrix}\n",
    "            \\alpha '(z_{11}) & \\dots & \\alpha '(z_{1p_i})\\\\\n",
    "             & \\ddots & \\vdots \\\\\n",
    "            \\alpha '(z_{n1}) & & \\alpha '(z_{np_i}) \n",
    "       \\end{bmatrix}\\in R^{p_i\\times n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "其余部分和之前的相同\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    &\\frac{\\triangledown Z_i}{\\triangledown W_i} = A_{i-1}\\in R^{p_{i-1}\\times n}\\\\\n",
    "    \\\\\n",
    "    &\\frac{\\triangledown Z_i}{\\triangledown A_{i-1}} = W_i^T\\in R^{p_{i-1}\\times i}\\\\\n",
    "    \\\\\n",
    "    &\\frac{\\triangledown Z_i}{\\triangledown b_i} = 1^{1\\times n} \\in R^{1\\times n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "将其和之前的梯度结合起来：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\triangledown loss}{\\triangledown W_i} &= \\left(\\frac{\\triangledown loss}{\\triangledown A_i}*\\frac{\\triangledown A_i}{\\triangledown Z_i}\\right)\\left(\\frac{\\triangledown Z_i}{\\triangledown W_i}\\right)^T \\in R^{p_i\\times p_{i-1}}\\\\\n",
    "    \\frac{\\triangledown loss}{\\triangledown A_{i-1}} &= \\frac{\\triangledown Z_i}{\\triangledown A_{i-1}}\\left(\\frac{\\triangledown loss}{\\triangledown A_i}*\\frac{\\triangledown A_i}{\\triangledown Z_m}\\right)\\in R^{p_{i-1}\\times n}\\\\\n",
    "    \\frac{\\triangledown loss}{\\triangledown b_i} &= \\left(\\frac{\\triangledown loss}{\\triangledown A_i}*\\frac{\\triangledown A_i}{\\triangledown Z_i}\\right)1^{n\\times 1}\\in R^{p_i\\times 1}\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.从第一层到输入层\n",
    "\n",
    "从第1层到输入层的反向传播和从第i层到第i-1层的推导相似，区别在于输入是固定的数据，而不再是激活函数值，也就不再需要对输入的数据$X$进行更新：\n",
    "\n",
    "假设$\\frac{\\triangledown loss}{\\triangledown A_1}\\in R^{p_i\\times n}$已知，\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    A_1 &= \\begin{bmatrix}\n",
    "            \\alpha (z_{11}) & \\dots & \\alpha (z_{1p_1})\\\\\n",
    "             & \\ddots & \\vdots \\\\\n",
    "            \\alpha (z_{n1}) & &\\alpha(z_{np_1}) \n",
    "          \\end{bmatrix}\\in R^{p_i\\times n}\\\\\n",
    "       \\frac{\\triangledown A_i}{\\triangledown Z_i}&=\\begin{bmatrix}\n",
    "            \\alpha '(z_{11}) & \\dots & \\alpha '(z_{1p_1})\\\\\n",
    "             & \\ddots & \\vdots \\\\\n",
    "            \\alpha '(z_{n1}) & & \\alpha '(z_{np_1})\n",
    "       \\end{bmatrix}\\in R^{p_i\\times n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    &\\frac{\\triangledown Z_1}{\\triangledown W_1} = X \\in R^{p\\times n}\\\\\n",
    "    &\\frac{\\triangledown Z_1}{\\triangledown b_1} = 1^{1\\times n} \\in R^{1\\times n}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\triangledown loss}{\\triangledown W_1} &= \\left(\\frac{\\triangledown loss}{\\triangledown A_1}*\\frac{\\triangledown A_1}{\\triangledown Z_i}\\right)\\left(\\frac{\\triangledown Z_i}{\\triangledown W_i}\\right)^T \\in R^{p_1\\times p}\\\\\n",
    "    \\frac{\\triangledown loss}{\\triangledown b_1} &= \\left(\\frac{\\triangledown loss}{\\triangledown A_1}*\\frac{\\triangledown A_1}{\\triangledown Z_i}\\right)1^{n\\times 1}\\in R^{p_1\\times 1}\\\\\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、优化器\n",
    "\n",
    "优化器是指优化得到参数的方法，优化器基本都是基于梯度下降方法。如果你在线性回归中不用正规方程求解参数，而是用梯度下降，你会发现随着梯度不断下降，**梯度不断减小**。而这还不是最麻烦的问题，由于线性回归是凸优化，用梯度下降总会收敛到最小值，而神经网络多是非凸问题，梯度下降很可能会困在局部极值**无法收敛**。而且通常神经网络需要很多的数据进行训练，如果每次都像传统的梯度下降那样把所有数据都传入模型，则**计算成本很大**。\n",
    "\n",
    "这里先介绍**SGD**（Stochastic Gradient Descnet，随机梯度下降）优化器。\n",
    "\n",
    "SGD不再把所有的数据都用来进行梯度下降，而是只用小批量（**mini batch**）数据进行梯度下降，常见的选择是从2的4次方（16）到2的10次方之间，选用2的整数次方是根据计算机比特的特点决定的，而之前推导中梯度进行标准化时除以样本数，此时需要除以一批量的样本数。\n",
    "\n",
    "控制梯度下降停止的条件也有所改变，由于神经网络强大的非线性组合能力，训练到收敛会造成过拟合，于是神经网络中用到最多的是早停法，也即小批量进行训练时将全部样本循环数遍（**epoch**）后就立即停下，避免过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-10T10:41:31.383996Z",
     "start_time": "2019-03-10T10:41:26.633761Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
