{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯（Naive Bayes）\n",
    "\n",
    "## 一、概念\n",
    "\n",
    "朴素贝叶斯和高斯判别分析一样，都是生成学习算法，通过学习条件概率来达到分类判别的作用。\n",
    "\n",
    "先前对高斯判别分析进行前提假定时，假定给定Y时X的分布是多元正态分布，这也就意味着高斯判别分析适用于连续性数值变量（数据符合正态分布则模型效果更加）。而朴素贝叶斯则是用于离散变量的生成学习算法，朴素贝叶斯曾广泛用在垃圾邮件识别中，通过统计各个词汇是否出现来判断一个邮件是否是垃圾邮件。\n",
    "\n",
    "在朴素贝叶斯模型中，自变量X是一串取值为0或1的向量，向量长度代表字典中所有词汇的个数，每个位置对应一个单词，该单词出现在邮件中则为1，不出现则为0。假设在给定Y的条件下各个单词是否出现在邮件中是相互独立的，字典中共有5000个词，那么将有如下等式：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    p(x_1,...,x_{5000}|y) &= p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)...p(x_{5000}|y,x_1,x_2,...,x_{4999})\\\\\n",
    "    &= p(x_1|y)p(x_2|y)p(x_3|y)...p(x_{5000}|y) \\\\\n",
    "    &= \\prod_{i=1}^{5000}p(x_i|y)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "第一行公式是按照条件概率的公式进行展开，第二行公式是运用假定——各个单词是否出现相互独立，得来的。\n",
    "\n",
    "假设在给定Y的条件下X的分布是伯努利分布：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    P(x_i=1|y=0) = \\phi_{i|y=0}\\\\\n",
    "    P(x_i=0|y=0) = 1-\\phi_{i|y=0}\\\\\n",
    "    P(x_i=1|y=1) = \\phi_{i|y=1}\\\\\n",
    "    P(x_i=0|y=0) = 1-\\phi_{i|y=1}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Y本身的分布也是伯努利分布：\n",
    "\n",
    "$$\n",
    "p(y) = \\phi^y(1-\\phi)^{(1-y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T10:48:13.337598Z",
     "start_time": "2019-02-26T10:48:13.327599Z"
    }
   },
   "source": [
    "## 二、优化\n",
    "\n",
    "朴素贝叶斯的优化同样是通过极大化似然函数，根据前述假定，似然函数为：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "L(\\phi_y, \\phi_{j|y=0}, \\phi_{j|y=1}) &= \\prod_{i=1}^n p(x^{(i)},y^{(i)})\\\\\n",
    "    &= \\prod_{i=1}^n p(x_1^{(i)},...,x_{5000}^{(i)}|y^{(i)})p(y^{(i)})\\\\\n",
    "    &= \\prod_{i=1}^n \\left[ \\prod_{j=1}^{5000}p(x_j^{(i)}|y^{(i)}) \\right]p(y^{(i)})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "之前$x_j$表示字典中的第j个词是否出现，为了不造成混淆，此处用$x^{(i)}$表示第i个样本。\n",
    "\n",
    "取对数之后易得使似然函数最大的参数值为：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\phi_{j|y=1}&=\\frac{\\sum_{i=1}^n I(x_j^{(i)} = 1 \\land y^{(i)}=1)}{\\sum_{i=1}^n I(y^{(i)}=1)}\\\\\n",
    "\\phi_{j|y=0}&=\\frac{\\sum_{i=1}^n I(x_j^{(i)} = 1 \\land y^{(i)}=0)}{\\sum_{i=1}^n I(y^{(i)}=0)}\\\\\n",
    "\\phi_y &= \\frac{\\sum_{i=1}^n I(y^{(i)}=1)}n\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "有了上述的参数之后，我们预测一封邮件是否是垃圾邮件时就可以直接用贝叶斯公式：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    P(y=1|x) &= \\frac{P(x|y=1)P(y=1)}{P(x)} \\\\\n",
    "    &= \\frac{(\\prod_{i=1}^n P(x_i|y=1))P(y=1)}{(\\prod_{i=1}^n P(x_i|y=1))P(y=1)+(\\prod_{i=1}^n P(x_i|y=0))P(y=0)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "然而实际应用时，经常会有一个单词在垃圾邮件和非垃圾邮件中都没出现，也就是:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\phi_{j|y=1}=\\frac{\\sum_{i=1}^n I(x_j^{(i)} = 1 \\land y^{(i)}=1)}{\\sum_{i=1}^n I(y^{(i)}=1)} = 0\\\\\n",
    "\\phi_{j|y=0}=\\frac{\\sum_{i=1}^n I(x_j^{(i)} = 1 \\land y^{(i)}=0)}{\\sum_{i=1}^n I(y^{(i)}=0)} = 0\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "这也就使得在应用贝叶斯定理时造成：\n",
    "\n",
    "$$\n",
    "P(y=1|x) = \\frac{(\\prod_{i=1}^nP(x_i|y=1))P(y=1)}{(\\prod_{i=1}^n P(x_i|y=1))P(y=1)+(\\prod_{i=1}^n P(x_i|y=0))P(y=0)} = \\frac 0 0\n",
    "$$\n",
    "\n",
    "这显然是我们需要规避的问题，于是便有了**拉普拉斯平滑（Laplace smoothing）**。拉普拉斯平滑的想法就是在所有的参数的分子和分母同时加上一个常数，使得$\\phi$不会为0。\n",
    "\n",
    "对于K分类问题通常是分母加K，分子加1：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\phi_m &= \\frac{\\sum_{i=1}^n I(y^{(i)}=m) + 1}{n+k} \\\\\n",
    "\\phi_{j|y=m} &= \\frac{\\sum_{i=1}^n I(x_j^{(i)} = 1 \\land y^{(i)}=m)+1}{\\sum_{i=1}^n I(y^{(i)}=m)+k}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "在2分类中把K变成2，m的取值为0-1即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、应用\n",
    "\n",
    "因为要求变量为离散的，所以直接适用于进行朴素贝叶斯分类的数据并不多，所以这次用生成的数据进行演示。\n",
    "\n",
    "在实际应用中可以对连续的数据进行分组离散化来应用朴素贝叶斯。\n",
    "\n",
    "### 1.生成数据\n",
    "\n",
    "首先生成取值在1-100的10000\\*500的随机整数矩阵，把1-1000的整数视为字典，统计字典中的单词是否出现，整理成10000\\*1000的矩阵。然后按照7：3划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T13:55:35.889693Z",
     "start_time": "2019-02-26T13:47:54.931979Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(2099)\n",
    "origin = np.random.randint(1, 1000, (10000, 500))\n",
    "\n",
    "data = np.zeros(1000).reshape([1, 1000])\n",
    "for i in range(10000):\n",
    "    tmp = origin[i,:].copy()\n",
    "    tmp = np.unique(tmp)\n",
    "    tmp2 = np.zeros(1000).reshape([1, 1000])\n",
    "    tmp2[:,tmp] = 1\n",
    "    data = np.vstack([data, tmp2])\n",
    "\n",
    "data = np.delete(data, 0, 0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助回归的形式（其实是感知机，perceptron）将化成取值0-1的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:32:39.397011Z",
     "start_time": "2019-02-26T14:32:39.328008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.random.randn(1000).reshape([1000,1])\n",
    "y = np.dot(data, tmp)\n",
    "y = y>y.mean()\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:32:46.713429Z",
     "start_time": "2019-02-26T14:32:46.702452Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:39:58.406790Z",
     "start_time": "2019-02-26T14:39:58.400790Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x = data[0:7000, :]\n",
    "train_y = y[0:7000, :]\n",
    "test_x = data[7000:10000, :]\n",
    "test_y = y[7000:10000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.求解\n",
    "\n",
    "直接应用拉普拉斯平滑计算$\\phi_{j|y=1}$和$\\phi_{j|y=0}$两个向量。当且仅当单词出现且是垃圾邮件时，代表两个变量的数值的和为2；当且仅当单词出现且不是垃圾邮件时，前者减去后者的差为1。以此可以方便计算和统计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T14:40:58.422918Z",
     "start_time": "2019-02-26T14:40:58.414918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3488\n",
      "3512\n"
     ]
    }
   ],
   "source": [
    "n_one = np.sum(train_y)\n",
    "n_zero = train_y.shape[0] - n_one\n",
    "print(n_one)\n",
    "print(n_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T15:14:40.825898Z",
     "start_time": "2019-02-26T15:14:40.526945Z"
    }
   },
   "outputs": [],
   "source": [
    "phi_1 = (np.sum(((train_x+train_y)==2), axis=0)+1)/(n_one+2)\n",
    "phi_0 = (np.sum(((train_x-train_y)==1), axis=0)+1)/(n_zero+2)\n",
    "phi_y = np.sum(train_y)/(train_y.shape[0]+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行预测较复杂，因为需要元素累乘，这种情况可以将向量处理成对角矩阵然后求行列式。\n",
    "\n",
    "$X*\\phi_1$和$(1-X)*(1-\\phi_1)$进行元素间的乘法，得到两个矩阵，计算每行的累乘即是$\\prod_{i=1}^nP(x_i|y=1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T19:04:52.706670Z",
     "start_time": "2019-02-26T18:46:41.015887Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(x, phi_0, phi_1, phi_y):\n",
    "    n = x.shape[0]\n",
    "    x_1 = x*phi_1 + (1-x)*(1-phi_1)\n",
    "    x_0 = x*phi_0 + (1-x)*(1-phi_0)\n",
    "    tmp_1 = None\n",
    "    tmp_0 = None\n",
    "    for i in range(n):\n",
    "        tmp = np.linalg.det(np.diag(x_1[i, :]))\n",
    "        tmp_1 = np.vstack([tmp_1, tmp])\n",
    "        tmp = np.linalg.det(np.diag(x_0[i, :]))\n",
    "        tmp_0 = np.vstack([tmp_0, tmp])\n",
    "    \n",
    "    tmp_1 = np.delete(tmp_1, 0, 0)\n",
    "    tmp_0 = np.delete(tmp_0, 0, 0)\n",
    "    return (tmp_1*phi_y)/(tmp_1*phi_y+tmp_0*(1-phi_y))\n",
    "\n",
    "train_y_hat = predict(train_x, phi_0, phi_1, phi_y)\n",
    "test_y_hat = predict(test_x, phi_0, phi_1, phi_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据似然函数的形式，可知似然函数是很多个小于1的数相乘的结果，实际似然函数极小，可能接近零，所以这里为了方便评价和比较训练集和测试集，依然选用交叉熵作为损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-26T19:24:40.720283Z",
     "start_time": "2019-02-26T19:24:40.703724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28723441]]\n",
      "[[0.37788191]]\n"
     ]
    }
   ],
   "source": [
    "def loss(y, y_hat):\n",
    "    return -(np.dot(y.T, np.log(y_hat)) + np.dot(1-y.T, np.log(1-y_hat)))/y.shape[0]\n",
    "\n",
    "train_y_hat = train_y_hat.astype('float64')\n",
    "test_y_hat = test_y_hat.astype('float64')\n",
    "\n",
    "print(loss(train_y, train_y_hat))\n",
    "print(loss(test_y, test_y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "差距还是挺大的，可能和数据生成有关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.86000000000001,
   "position": {
    "height": "40px",
    "left": "auto",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
